---
url: /guide/extensions/mp3-encoder.md
---
# @mediabunny/mp3-编码器

浏览器通常在 WebCodecs 实现中不支持 MP3 编码。鉴于该格式的普及性，此扩展包为 Mediabunny 提供了一个 MP3 编码器。它是通过 Mediabunny 的[自定义编码器 API](../supported-formats-and-codecs#custom-coders)实现的，底层使用了[LAME MP3 编码器](https://lame.sourceforge.io/)的高性能 WASM 构建版本。

## 安装

本库需要 Mediabunny 作为对等依赖。使用 npm 安装两者：

```bash
npm install mediabunny @mediabunny/mp3-encoder
```

或者，直接通过 script 标签引入：

```html
<script src="mediabunny.js"></script>
<script src="mediabunny-mp3-encoder.js"></script>
```

这将暴露全局对象 `Mediabunny` 和 `MediabunnyMp3Encoder`。使用 `mediabunny-mp3-encoder.d.ts` 为这些全局对象提供类型声明。您可以从[发布页面](https://github.com/Vanilagy/mediabunny/releases)下载构建好的分发文件。

## 使用方法

```ts
import { registerMp3Encoder } from '@mediabunny/mp3-encoder';

registerMp3Encoder();
```

就这样 - Mediabunny 现在会自动使用注册的 MP3 编码器。

如果想更规范，可以先检查浏览器原生支持情况：

```ts
import { canEncodeAudio } from 'mediabunny';
import { registerMp3Encoder } from '@mediabunny/mp3-encoder';

if (!(await canEncodeAudio('mp3'))) {
    registerMp3Encoder();
}
```

## 示例

这里我们将一个输入文件转换为 MP3 格式：

```ts
import {
    Input,
    ALL_FORMATS,
    BlobSource,
    Output,
    BufferTarget,
    Mp3OutputFormat,
    canEncodeAudio,
    Conversion,
} from 'mediabunny';
import { registerMp3Encoder } from '@mediabunny/mp3-encoder';

if (!(await canEncodeAudio('mp3'))) {
    // 仅在无原生支持时注册自定义编码器
    registerMp3Encoder();
}

const input = new Input({
    source: new BlobSource(file), // 例如来自文件选择器
    formats: ALL_FORMATS,
});
const output = new Output({
    format: new Mp3OutputFormat(),
    target: new BufferTarget(),
});

const conversion = await Conversion.init({
    input,
    output,
});
await conversion.execute();

output.target.buffer; // => 包含MP3文件的ArrayBuffer
```

## 实现细节

该库通过向 Mediabunny 注册自定义编码器类来实现 MP3 编码功能。初始化时，该类会启动一个 worker 线程，该线程立即加载 LAME MP3 编码器的 WASM 版本。原始数据被发送到 worker 线程进行编码，编码后的数据再返回主线程。这些编码后的数据块在主线程中被拼接并正确分割为独立的 MP3 帧。

我们特别注重确保该包的最大兼容性：它可与打包工具配合使用，直接在浏览器中运行，也支持 Node、Deno 和 Bun 环境。所有代码（包括 worker 和 WASM）都被打包成单个文件，无需 CDN 或 WASM 路径参数。因此，该包可作为 Mediabunny 基于 WASM 的编码器扩展的参考实现。

WASM 构建本身是经过性能优化的、启用 SIMD 的 LAME 3.100 版本，所有不需要的功能都被禁用。由于性能最大化是首要目标，构建体积略大，但经过 gzip 压缩后约 130 kB 的大小在我看来仍然非常合理。在我的测试中，编码 5 秒音频约需 90 毫秒（达到实时速度的 55 倍）。

---

---
url: /llms.md
---
# Mediabunny 与大型语言模型

虽然 Mediabunny 自豪地由人类创作，但我们鼓励任何形式的使用，即使是在氛围高涨时也不例外。

Mediabunny 仍然很新，不太可能出现在现代大型语言模型的训练数据中，但只需提供更多上下文信息，我们就能让 AI 表现出色。

***

将以下一个或多个文件提供给您的 LLM：

### [mediabunny.d.ts](/mediabunny.d.ts)

该文件包含 Mediabunny 的全部公共 TypeScript API，并附有极其详尽的注释。

### [llms.txt](/llms.txt)

该文件提供了 Mediabunny 指南的索引，AI 可以根据需要进一步深入查看。

### [llms-full.txt](/llms-full.txt)

这是完整的 Mediabunny 指南，全部内容都在单个文件中。

---

---
url: /guide/writing-media-files.md
---
# 写入媒体文件

Mediabunny 使您能够以非常精细的控制级别创建媒体文件。您可以向媒体文件中添加任意数量的视频、音频和字幕轨道，并精确控制媒体数据的时间安排。该库支持[多种输出文件格式](./output-formats)。通过使用[输出目标](#output-targets)，您可以选择是将整个文件构建在内存中，还是在创建过程中以数据块形式流式输出——这使得您可以创建非常大的文件。

Mediabunny 提供了多种方式来为输出轨道提供媒体数据，与 WebCodecs API 良好集成，同时也允许您根据需要选择自己的编码堆栈。这些[媒体源](./media-sources)具有多个抽象级别，既便于常见用例的简单使用，又能在需要时提供细粒度的控制。

## 创建输出

在 Mediabunny 中，媒体文件创建围绕一个核心类 `Output` 展开。一个 `Output` 实例代表您要创建的一个媒体文件。

首先使用您想要创建的文件的配置创建一个新的 `Output` 实例：

```ts
import { Output, Mp4OutputFormat, BufferTarget } from 'mediabunny';

// 在这个例子中，我们将在内存中创建一个 MP4 文件：
const output = new Output({
	format: new Mp4OutputFormat(),
	target: new BufferTarget(),
});
```

查看[输出格式](./output-formats)获取可用输出格式的完整列表。\
查看[输出目标](#output-targets)获取可用输出目标的完整列表。

您可以随时访问输出对象的 `format` 和 `target` 属性：

```ts
output.format; // => Mp4OutputFormat
output.target; // => BufferTarget
```

## 添加轨道

`Output` 对象提供了几个方法来添加轨道：

```ts
output.addVideoTrack(videoSource);
output.addAudioTrack(audioSource);
output.addSubtitleTrack(subtitleSource);
```

对于每个要添加的轨道，你需要为其创建一个唯一的[媒体源](./media-sources)。通过这些媒体源，你可以向输出添加媒体数据。一个媒体源只能用于一个输出轨道。

可选地，你可以在添加轨道时指定额外的轨道元数据：

```ts
// 指定视频轨道在视频播放器中显示前应顺时针旋转90度
// 并预期帧率为30 FPS
output.addVideoTrack(videoSource, {
	// 顺时针旋转角度（度）
	rotation: 90,
	// 预期帧率（赫兹）
	frameRate: 30,
});

// 添加两个音频轨道：英语和德语
output.addAudioTrack(audioSourceEng, {
	language: 'eng', // ISO 639-2/T 语言代码
	name: '开发者评论', // 设置用户定义的轨道名称
});
output.addAudioTrack(audioSourceGer, {
	language: 'ger',
});

// 添加多个字幕轨道，支持不同语言
output.addSubtitleTrack(subtitleSourceEng, { language: 'eng' });
output.addSubtitleTrack(subtitleSourceGer, { language: 'ger' });
output.addSubtitleTrack(subtitleSourceSpa, { language: 'spa' });
output.addSubtitleTrack(subtitleSourceFre, { language: 'fre' });
output.addSubtitleTrack(subtitleSourceIta, { language: 'ita' });
```

::: info
可选的 `frameRate` 视频轨道元数据选项指定了视频的预期帧率。添加到该轨道的所有帧的时间戳和持续时间都将对齐到指定的帧率。你应该避免以高于该帧率的频率添加帧，否则会导致多个帧具有相同的时间戳。

要精确实现常见的分数帧率，请确保使用其精确分数形式：
$23.976 \rightarrow 24000/1001$\
$29.97 \rightarrow 30000/1001$\
$59.94 \rightarrow 60000/1001$
:::

举个例子，我们向输出添加两个轨道：

* 一个由 `<canvas>` 元素内容驱动的视频轨道，使用 AVC 编码
* 一个由用户麦克风输入驱动的音频轨道，使用 AAC 编码

```ts
import { CanvasSource, MediaStreamAudioTrackSource } from 'mediabunny';

// 假设 `canvasElement` 存在
const videoSource = new CanvasSource(canvasElement, {
	codec: 'avc',
	bitrate: 1e6, // 1 Mbps
});

const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
const audioStreamTrack = stream.getAudioTracks()[0];
const audioSource = new MediaStreamAudioTrackSource(audioStreamTrack, {
	codec: 'aac',
	bitrate: 128e3, // 128 kbps
});

output.addVideoTrack(videoSource, { frameRate: 30 });
output.addAudioTrack(audioSource);
```

::: warning
如果轨道与输出格式不兼容，向 `Output` 添加轨道会抛出错误。添加轨道时请务必遵守输出格式的[属性](./output-formats#format-properties)。
:::

## 设置元数据标签

Mediabunny 允许你向输出文件写入额外的描述性元数据标签，例如标题、艺术家或封面艺术：

```ts
output.setMetadataTags({
	title: 'Big Buck Bunny',
	artist: 'Blender Foundation',
	date: new Date('2008-05-20'),
	images: [{
		data: new Uint8Array([...]),
		mimeType: 'image/jpeg',
		kind: 'coverFront',
	}],
});
```

有关可写入标签的更多信息，请参阅 [`MetadataTags`](../api/MetadataTags)。

## 启动输出

将所有轨道添加到 `Output` 后，你需要*启动*它。启动输出会开始写入过程，允许你现在开始向输出文件发送媒体数据。同时这也将阻止你向其添加任何新轨道。

```ts
await output.start(); // 当输出准备好接收媒体数据时解析
```

## 添加媒体数据

启动 `Output` 后，你可以使用添加轨道时使用的媒体源将媒体数据传输到输出文件。每个[媒体源](./media-sources)的 API 有所不同，但通常看起来像这样：

```ts
mediaSource.add(...);
```

在我们的示例中，一旦调用 `start`，用户的麦克风输入就会被传输到输出文件。但我们仍然需要添加来自画布的数据。我们可以这样做：

```ts
let framesAdded = 0;
const intervalId = setInterval(() => {
	const timestampInSeconds = framesAdded / 30;
	const durationInSeconds = 1 / 30;

	// 捕获调用 `add` 时的画布状态：
	videoSource.add(timestampInSeconds, durationInSeconds);
	framesAdded++;
}, 1000 / 30);
```

然后让这个循环运行，持续捕获我们想要的媒体数据时长。

## 完成输出

当所有媒体数据都添加完毕后，需要对 `Output` 进行*最终化*处理。最终化操作会完成所有剩余的编码工作，并写入剩余数据以生成最终可播放的媒体文件。

```ts
await output.finalize(); // 在输出完成最终化后解析
```

::: warning
调用 `finalize` 后，再向输出添加更多媒体数据会导致错误。
:::

在我们的示例中，需要这样做：

```ts
clearInterval(intervalId); // 停止画布循环
audioStreamTrack.stop(); // 停止捕获用户麦克风

await output.finalize();

const file = output.target.buffer; // => Uint8Array
```

## 取消输出

有时您可能需要取消正在进行的输出文件创建。此时可以使用 `cancel` 方法：

```ts
await output.cancel(); // 在输出取消后解析
```

该方法会自动释放输出过程中使用的所有资源，例如关闭所有编码器或释放写入器。

::: warning
调用 `cancel` 后，再向输出添加更多媒体数据会导致错误。
:::

在我们的示例中，可以这样操作：

```ts
clearInterval(intervalId); // 停止画布循环
audioStreamTrack.stop(); // 停止捕获用户麦克风

await output.cancel();

// 输出已取消
```

## 检查输出状态

您可以通过 `state` 属性随时检查输出的当前状态：

```ts
output.state; // => 'pending' | 'started' | 'canceled' | 'finalizing' | 'finalized'
```

* `'pending'` - 输出尚未开始或取消；可以添加新轨道。
* `'started'` - 输出已开始并准备接收媒体数据；不能再添加轨道。
* `'finalizing'` - 已调用 `finalize` 但尚未解析；不能再添加媒体数据。
* `'finalized'` - 输出已完成最终化并完成文件写入。
* `'canceled'` - 输出已被取消。

## 输出目标

*output target*（输出目标）决定了 `Output` 创建的数据将被写入的位置。本库提供了几种目标类型。

如果您的输出格式配置需要进行数据包缓冲，请确保以交错方式添加媒体数据以保持较低的内存使用率。例如，如果要创建5分钟的文件，应该分块添加数据——先添加10秒视频，再添加10秒音频，然后重复——而不是先添加全部300秒视频再添加全部300秒音频。

::: info
如果您的使用场景无法实现这种分块处理，可以尝试先添加数据量较小的媒体：先添加300秒音频，再添加300秒视频。
:::

## 输出 MIME 类型

有时您可能需要获取 `Output` 创建文件的 MIME 类型。例如，在使用 Media Source Extensions 时，[`addSourceBuffer`](https://developer.mozilla.org/en-US/docs/Web/API/MediaSource/addSourceBuffer) 需要文件的完整 MIME 类型，包括编解码器字符串。

为此，可以使用以下方法：

```ts
output.getMimeType(); // => Promise<string>
```

该方法可能返回如下字符串：

```
video/mp4; codecs="avc1.42c032, mp4a.40.2"
```

::: warning
`getMimeType` 返回的 Promise 只有在 `Output` 所有轨道的精确编解码器字符串都已知时才会解析——这意味着它可能需要等待所有编码器完全初始化。因此，请确保不要陷入死锁：在向轨道添加媒体数据之前等待此方法将导致 Promise 永远无法解析。
:::

如果您不关心具体的轨道编解码器，可以直接使用 `Output` 格式上更简单的 [`mimeType`](./output-formats#output-format-properties) 属性：

```ts
output.format.mimeType; // => string
```

***

所有目标(target)都有一个可选的 `onwrite` 回调函数，可用于监控哪些字节区域正在被写入：

```ts
target.onwrite = (start, end) => {
	// ...
};
```

你可以使用这个回调来追踪输出文件的大小变化。但请注意，这个函数会被频繁调用，调用次数*极其*多。

### `BufferTarget`

该目标将所有数据写入一个连续的内存中的 `ArrayBuffer`。这个缓冲区会随着文件增大而自动扩容。使用方法很简单：

```ts
import { Output, BufferTarget } from 'mediabunny';

const output = new Output({
	target: new BufferTarget(),
	// ...
});

// ...

output.target.buffer; // => null
await output.finalize();
output.target.buffer; // => ArrayBuffer
```

这个目标非常适合处理较小的文件（< 100 MB），但由于所有数据都会保存在内存中，处理大文件时就不太理想。如果输出非常大，页面可能会因内存耗尽而崩溃。对于这种情况，建议使用 `StreamTarget`。

### `StreamTarget`

该目标将 `Output` 写入的数据分成小块传递给你，需要你将这些数据管道传输到其他地方手动组装成最终文件。典型用例包括直接将文件写入磁盘，或通过网络上传到服务器。

`StreamTarget` 使用了 Streams API，这意味着你需要传递一个 `WritableStream` 实例：

```ts
import { Output, StreamTarget, StreamTargetChunk } from 'mediabunny';

const writable = new WritableStream({
	write(chunk: StreamTargetChunk) {
		chunk.data; // => Uint8Array
		chunk.position; // => number

		// 对数据进行处理...
	}
});

const output = new Output({
	target: new StreamTarget(writable),
	// ...
});
```

写入 `WritableStream` 的每个块(chunk)代表输出文件的一个连续字节区域 `data`，这些数据应该被写入给定的字节偏移量 `position`。当在 `Output` 上调用 `finalize` 或 `cancel` 时，`WritableStream` 会自动关闭。

::: warning
注意输出文件中的某些字节区域可能会被多次写入。因此，简单地拼接所有 `Uint8Array` 来构建最终文件是**错误的**——你**必须**按照块到达的顺序，在每个指定的字节偏移位置写入数据。如果不这样做，输出文件可能会无效或损坏。

某些[输出格式](./output-formats)具有*仅追加*的写入模式，在这种模式下，写入块的字节偏移量总是等于之前所有写入块的总字节数。换句话说，当写入是仅追加时，简单地拼接所有 `Uint8Array` 就能得到正确结果。某些 API（如 Media Source Extensions 的 `appendBuffer`）需要这种模式，因此在这些情况下请确保相应配置输出格式。
:::

#### 分块模式

默认情况下，`StreamTarget` 会在数据可用时立即发出。在某些格式中，这可能导致每秒数百次写入事件。如果你想减少写入频率，`StreamTarget` 提供了一个替代的"分块模式"，在这种模式下，数据会先在内存中累积成大块，只有当块完全填满时才会发出。

```ts
new StreamTarget(writable, {
	chunked: true,
	chunkSize: 2 ** 20, // 可选；默认为 16 MiB
}),
```

#### 应用背压

有时，`Output` 产生新数据的速度可能比你写入的速度快。在这种情况下，你需要告诉 `Output` 应该"冷静下来"，放慢速度以匹配 `WritableStream` 的处理速度。使用 `StreamTarget` 时，`Output` 会自动尊重 `WritableStream` 应用的背压。为此，了解[如何应用背压的 Stream API 概念](https://developer.mozilla.org/en-US/docs/Web/API/Streams_API/Concepts)很有帮助。

例如，可写流可以通过在 `write` 中返回一个 promise 来应用背压：

```ts
const writable = new WritableStream({
	write(chunk: StreamTargetChunk) {
		// 假设写出数据需要 10 毫秒：
		return new Promise(resolve => setTimeout(resolve, 10));
	}
});
```

::: info
为了使可写流的背压能够贯穿整个管道，你必须确保正确遵守[媒体源应用的背压](./media-sources#backpressure)。
:::

#### 与文件系统 API 一起使用

`StreamTargetChunk` 的设计使其与文件系统 API 的 `FileSystemWritableFileStream` 兼容。这意味着如果你想直接将数据写入磁盘，可以简单地这样做：

```ts
const handle = await window.showSaveFilePicker();
const writableStream = await handle.createWritable();

const output = new Output({
	target: new StreamTarget(writableStream),
	// ...
});

// ...

await output.finalize(); // 会自动关闭可写流
```

### `NullTarget`

该目标简单地丢弃所有传入的数据。当你需要一个 `Output` 但通过其他方式（如特定输出格式的回调或编码器事件）提取数据时，它很有用。

例如，这里我们创建一个分段的 MP4 文件并直接处理各个片段：

```ts
import { Output, NullTarget, Mp4OutputFormat } from 'mediabunny';

let ftyp: Uint8Array;
let lastMoof: Uint8Array;

const output = new Output({
	target: new NullTarget(),
	format: new Mp4OutputFormat({
		fastStart: 'fragmented',
		onFtyp: (data) => {
			ftyp = data;
		},
		onMoov: (data) => {
			const header = new Uint8Array(ftyp.length + data.length);
			header.set(ftyp, 0);
			header.set(data, ftyp.length);

			// 对头部数据进行处理...
		},
		onMoof: (data) => {
			lastMoof = data;
		},
		onMdat: (data) => {
			const segment = new Uint8Array(lastMoof.length + data.length);
			segment.set(lastMoof, 0);
			segment.set(data, lastMoof.length);

			// 对片段数据进行处理...
		},
	}),
});
```

## 数据包缓冲

某些[输出格式](./output-formats)在多轨道输出时需要*数据包缓冲*。数据包缓冲的发生是因为 `Output` 必须等待给定时间戳的所有轨道数据才能继续写入数据。例如，如果你先编码所有视频帧，然后再编码音频，`Output` 将不得不将所有视频帧保存在内存中，直到音频数据包开始传入。如果你的视频很长，这可能会导致内存耗尽。当只有一个媒体轨道时，这个问题不会出现。

查看[输出格式](./output-formats)页面了解哪些格式配置需要数据包缓冲。

***

---

---
url: /guide/media-sinks.md
---
# 媒体接收器

## 简介

*媒体接收器* 提供了从 `InputTrack` 中提取媒体数据的多种方式。不同的媒体接收器提供了不同层次的抽象，适用于不同的使用场景。

关于如何获取输入轨道，或如何从媒体文件中读取数据的一般信息，请参阅 [读取媒体文件](./reading-media-files)。

### 通用用法

> 以下将使用虚构的 `FooSink` 来演示媒体接收器的通用使用模式。

媒体接收器就像是用于检索媒体数据的微型"命名空间"，其作用域限定于特定轨道。这意味着通常每种类型的接收器只需为每个轨道构造一次。

```ts
const track = await input.getPrimaryVideoTrack();
const sink = new FooSink(track);
```

构造接收器几乎不需要任何开销，也不会执行任何媒体数据读取操作。

为了读取媒体数据，每个接收器都提供了不同的方法集。你可以根据需要多次调用这些方法；由于媒体接收器是无状态的\[^1]，这些调用彼此独立。

```ts
await sink.getFoo(1);
await sink.getFoo(2);
await sink.getFoo(3);
```

### 异步迭代器

媒体接收器（media sinks）大量使用了[异步迭代器](https://developer.mozilla.org/en-US/docs/Web/JavaScript/ReferenceGlobal_Objects/AsyncIterator)。它们允许你高效地遍历一组媒体数据（如视频轨道中的所有帧），在任何时刻只需读取文件的很小部分。

异步迭代器与 `for await...of` 循环搭配使用极为便捷：

```ts
for await (const foo of sink.foos()) {
	console.log(foo.timestamp);
}
```

就像常规的 `for` 循环一样，可以使用 `break` 语句提前退出循环。这将自动清理异步迭代器使用的所有内部资源（如解码器）：

```ts
// 仅遍历前5个foo
let count = 0;
for await (const foo of sink.foos()) {
	console.log(foo.timestamp);
	if (++count === 5) break;
}
```

异步迭代器在 `for` 循环之外也同样有用。这里使用 `next` 方法来获取迭代中的下一项：

```ts
const foos = sink.foos();

const foo1Result = await foos.next();
const foo2Result = await foos.next();

const foo1 = foo1Result.value; // 如果迭代完成，可能是 `undefined`
```

::: warning
手动使用异步迭代器时，请确保在使用完毕后调用 `return`：

```ts
await foos.return();
```

这能确保释放所有内部持有的资源。
:::

### 解码顺序 vs. 呈现顺序

文件中的数据包可能出现乱序，这意味着它们的解码顺序与解码后数据的显示顺序不一致（参见[B帧](./media-sources#b-frames)）。媒体接收器的方法在查询和检索数据包时使用的顺序有所不同。因此，请牢记以下定义：

* **呈现顺序（Presentation order）：** 数据被呈现的顺序；按时间戳排序。
* **解码顺序（Decode order）：** 数据包必须被解码的顺序；不一定按时间戳排序。

## 通用接收器

有一个媒体接收器可以与任何 `InputTrack` 配合使用：

### `EncodedPacketSink`

该 sink 可用于从媒体文件中提取原始的[编码数据包](./packets-and-samples#encodedpacket)，是最基础的媒体 sink。如果您不关心解码后的媒体数据（例如只对时间戳感兴趣），或者想要自行实现解码逻辑，`EncodedPacketSink` 会非常有用。

首先从任意 `InputTrack` 构造 sink：

```ts
import { EncodedPacketSink } from 'mediabunny';

const sink = new EncodedPacketSink(track);
```

您可以根据以秒为单位的时间戳检索特定数据包：

```ts
await sink.getPacket(5); // => EncodedPacket | null

// 或者只检索类型为 'key' 的数据包：
await sink.getKeyPacket(5); // => EncodedPacket | null
```

当使用时间戳检索数据包时，将返回时间戳小于或等于搜索时间戳的最后一个数据包（按[呈现顺序](#decode-vs-presentation-order)）。如果不存在这样的数据包，方法将返回 `null`。

有一个特殊方法用于检索第一个数据包（按[解码顺序](#decode-vs-presentation-order)）：

```ts
await sink.getFirstPacket(); // => EncodedPacket | null
```

最后一个数据包（按[呈现顺序](#decode-vs-presentation-order)）可以这样检索：

```ts
await sink.getPacket(Infinity); // => EncodedPacket | null
```

获取数据包后，可以这样检索其后继数据包（按[解码顺序](#decode-vs-presentation-order)）：

```ts
await sink.getNextPacket(packet); // => EncodedPacket | null

// 或者直接跳转到下一个类型为 'key' 的数据包：
await sink.getNextKeyPacket(packet); // => EncodedPacket | null
```

如果没有下一个数据包，这些方法将返回 `null`。

这些方法可以组合使用来遍历一系列数据包。从初始数据包开始，循环调用 `getNextPacket` 来遍历数据包：

```ts
let currentPacket = await sink.getFirstPacket();
while (currentPacket) {
	console.log('Packet:', currentPacket);
	// 对数据包进行处理

	currentPacket = await sink.getNextPacket(currentPacket);
}
```

虽然这种方法可行，但 `EncodedPacketSink` 还提供了一个专门的 `packets` 迭代器函数，按[解码顺序](#decode-vs-presentation-order)遍历数据包：

```ts
for await (const packet of sink.packets()) {
	// ...
}
```

您还可以使用数据包范围约束迭代，迭代将从起始数据包开始，直到（但不包括）结束数据包：

```ts
const start = await sink.getPacket(5);
const end = await sink.getPacket(10, { metadataOnly: true });

for await (const packet of sink.packets(start, end)) {
	// ...
}
```

`packets` 方法比手动迭代更高效，因为它会在需要之前智能地预加载未来的数据包。

#### 验证关键帧数据包

默认情况下，数据包类型是通过容器文件提供的元数据来确定的。某些文件可能会错误地将增量帧数据包标记为关键帧数据包，从而导致潜在的解码错误。为确保获取的数据包确实是关键帧数据包，您可以启用 `verifyKeyPackets` 选项：

```ts
// 如果此方法返回的数据包类型为 'key'，则可以确保它是关键帧数据包
await sink.getPacket(5, { verifyKeyPackets: true });

// 返回的数据包保证是关键帧数据包
await sink.getKeyPacket(10, { verifyKeyPackets: true });
await sink.getNextKeyPacket(packet, { verifyKeyPackets: true });

// 迭代器同样适用：
for await (const packet of sink.packets(
	undefined,
	undefined,
	{ verifyKeyPackets: true },
)) {
	// ...
}
```

::: info
`verifyKeyPackets` 仅在未同时启用 `metadataOnly` 时有效。
:::

#### 仅获取元数据的数据包

有时您可能只关心数据包的元数据（时间戳、时长、类型等），而不需要其编码的媒体数据。`EncodedPacketSink` 上的所有方法都接受一个最终的 `options` 参数，您可以用它来获取[仅含元数据的数据包](./packets-and-samples#metadata-only-packets)：

```ts
const packet = await sink.getPacket(5, { metadataOnly: true });

packet.isMetadataOnly; // => true
packet.data; // => Uint8Array([])
```

对于某些输入格式，获取仅含元数据的数据包效率更高：只需读取文件的元数据部分，而无需读取媒体数据部分。

## 视频数据接收器

这些接收器只能与 `InputVideoTrack` 一起使用。

### `VideoSampleSink`

使用此接收器(sink)可以从视频轨道中提取解码后的[视频样本](./packets-and-samples#videosample)（帧）。该接收器会自动在内部处理解码过程。

::: info
此接收器的所有操作都使用[呈现顺序](#decode-vs-presentation-order)。
:::

创建接收器的方式如下：

```ts
import { VideoSampleSink } from 'mediabunny';

const sink = new VideoSampleSink(videoTrack);
```

#### 单次获取

您可以获取指定时间戳（以秒为单位）呈现的样本：

```ts
await sink.getSample(5);

// 提取第一个样本：
await sink.getSample(await videoTrack.getFirstTimestamp());

// 提取最后一个样本：
await sink.getSample(Infinity);
```

该方法返回时间戳小于或等于搜索时间戳的最后一个样本，如果没有这样的样本则返回 `null`。

#### 范围迭代

您可以使用 `samples` 迭代器方法来遍历连续的样本范围：

```ts
// 遍历所有样本：
for await (const sample of sink.samples()) {
	console.log('Sample:', sample);
	// 对样本进行处理

	sample.close();
}

// 遍历特定时间范围内的所有样本：
for await (const sample of sink.samples(5, 10)) {
	// ...
	sample.close();
}
```

`samples` 迭代器按照[呈现顺序](#decode-vs-presentation-order)（按时间戳排序）生成样本。

#### 稀疏迭代

有时，您可能需要一次性获取多个时间戳的样本（例如用于生成缩略图）。虽然可以多次调用 `getSample`，但 `samplesAtTimestamps` 方法提供了更高效的方式：

```ts
for await (const sample of sink.samplesAtTimestamps([0, 1, 2, 3, 4, 5])) {
	// `sample` 可能是 VideoSample 或 null
	sample.close();
}

// 允许任意时间戳序列：
sink.samplesAtTimestamps([1, 2, 3]);
sink.samplesAtTimestamps([4, 5, 5, 5]);
sink.samplesAtTimestamps([10, -2, 3]);
```

此方法比多次调用 `getSample` 更高效，因为它避免了重复解码同一个数据包。

除了数组，您还可以传入任何可迭代对象：

```ts
sink.samplesAtTimestamps(new Set([2, 3, 3, 4]));

sink.samplesAtTimestamps((function* () {
	for (let i = 0; i < 5; i++) {
		yield i;
	}
})());

sink.samplesAtTimestamps((async function* () {
	const firstTimestamp = await videoTrack.getFirstTimestamp();
	const lastTimestamp = await videoTrack.computeDuration();

	for (let i = 0; i <= 100; i++) {
		yield firstTimestamp + (lastTimestamp - firstTimestamp) * i / 100;
	}
})());
```

当与 `EncodedPacketSink` 配合使用时，传入异步可迭代对象特别有用。假设您想获取每个关键帧，一个简单的实现可能如下：

```ts
// 简单但低效的实现： // [!code error]
const packetSink = new EncodedPacketSink(videoTrack);
const keyFrameTimestamps: number[] = [];

let currentPacket = await packetSink.getFirstPacket();
while (currentPacket) {
	keyFrameTimestamps.push(currentPacket.timestamp);
	currentPacket = await packetSink.getNextKeyPacket(currentPacket);
}

const sampleSink = new VideoSampleSink(videoTrack);
const keyFrameSamples = sampleSink.samplesAtTimestamps(keyFrameTimestamps);

for await (const sample of keyFrameSamples) {
	// ...
	sample.close();
}
```

这种实现的问题在于它会先遍历所有关键帧数据包，然后才生成第一个样本。更好的实现方式是：

```ts
// 优化后的实现：
const packetSink = new EncodedPacketSink(videoTrack);
const sampleSink = new VideoSampleSink(videoTrack);	

const keyFrameSamples = sampleSink.samplesAtTimestamps((async function* () {
	let currentPacket = await packetSink.getFirstPacket();

	while (currentPacket) {
		yield currentPacket.timestamp;
		currentPacket = await packetSink.getNextKeyPacket(currentPacket);
	}
})());

for await (const sample of keyFrameSamples) {
	// ...
	sample.close();
}
```

### `CanvasSink`

当 `VideoSampleSink` 提取原始解码的视频样本时，您可以使用 `CanvasSink` 将这些样本提取为画布。通过这种方式，某些操作（如缩放、旋转和裁剪）也可以由接收器处理。缺点是画布帧缓冲区的额外 VRAM 需求。

::: info
此接收器尽可能生成 `HTMLCanvasElement`，否则会回退到 `OffscreenCanvas`（例如在 Worker 上下文中）。
:::

创建接收器如下：

```ts
import { CanvasSink } from 'mediabunny';

const sink = new CanvasSink(videoTrack, options);
```

这里的 `options` 具有以下类型：

```ts
type CanvasSinkOptions = {
	width?: number;
	height?: number;
	fit?: 'fill' | 'contain' | 'cover';
	rotation?: 0 | 90 | 180 | 270;
	crop?: { left: number; top: number; width: number; height: number };
	poolSize?: number;
};
```

* `width`\
  输出画布的宽度（以像素为单位）。当省略但设置了 `height` 时，宽度将自动计算以保持原始宽高比。否则，宽度将设置为视频的原始宽度。
* `height`\
  输出画布的高度（以像素为单位）。当省略但设置了 `width` 时，高度将自动计算以保持原始宽高比。否则，高度将设置为视频的原始高度。
* `fit`\
  当同时设置了 `width` 和 `height` 时*必需*，此选项设置使用的拟合算法。
  * `'fill'` 将拉伸图像以填充整个框，可能会改变宽高比。
  * `'contain'` 将在保持宽高比的同时将整个图像包含在框内。这可能导致黑边。
  * `'cover'` 将缩放图像直到填充整个框，同时保持宽高比。
* `rotation`\
  用于旋转原始视频帧的顺时针旋转角度。默认为文件元数据中设置的旋转。旋转在裁剪和调整大小之前应用。
* `crop`\
  指定要裁剪的输入视频的矩形区域。裁剪区域将自动限制在输入视频轨道的尺寸内。裁剪在旋转之后但在调整大小之前执行。
* `poolSize`\
  参见 [Canvas pool](#canvas-pool)。

一些示例：

```ts
// 此接收器生成具有轨道未更改显示尺寸的画布，
// 并尊重轨道的旋转元数据。
new CanvasSink(videoTrack);

// 此接收器生成宽度为 1280 的画布，高度保持原始显示宽高比。
new CanvasSink(videoTrack, {
	width: 1280,
});

// 此接收器生成方形画布，视频帧缩放以完全覆盖画布。
new CanvasSink(videoTrack, {
	width: 512,
	height: 512,
	fit: 'cover',
});

// 此接收器生成具有轨道未更改编码尺寸的画布，
// 并且不应用任何旋转。
new CanvasSink(videoTrack, {
	rotation: 0,
});
```

检索画布的方法与 `VideoSampleSink` 上的方法类似：

* `getCanvas`\
  获取给定时间戳的画布；参见 [Single retrieval](#single-retrieval)。
* `canvases`\
  迭代一系列画布；参见 [Range iteration](#range-iteration)。
* `canvasesAtTimestamps`\
  迭代特定时间戳的画布；参见 [Sparse iteration](#sparse-iteration)。

这些方法生成 `WrappedCanvas` 实例：

```ts
type WrappedCanvas = {
	// 画布元素或离屏画布。
	canvas: HTMLCanvasElement | OffscreenCanvas;
	// 对应视频样本的时间戳，以秒为单位。
	timestamp: number;
	// 对应视频样本的持续时间，以秒为单位。
	duration: number;
};
```

#### 画布池

默认情况下，此接收器为每个生成的画布都会创建一个新的画布。如果你知道在任何时候只需要保留少量画布，应该使用 `poolSize` 选项。这个整数值指定了池中的画布数量；这些画布会以环形缓冲区/轮询的方式重复使用。这样可以保持分配的显存恒定，并避免浏览器频繁分配/释放画布。池大小为0或`undefined`时会禁用池功能。

使用池大小为3的示例：

```ts
const sink = new CanvasSink(videoTrack, { poolSize: 3 });

const a = await sink.getCanvas(42);
const b = await sink.getCanvas(42);
const c = await sink.getCanvas(42);
const d = await sink.getCanvas(42);
const e = await sink.getCanvas(42);
const f = await sink.getCanvas(42);

assert(a.canvas === d.canvas);
assert(b.canvas === e.canvas);
assert(c.canvas === f.canvas);
assert(a.canvas !== b.canvas);
assert(a.canvas !== c.canvas);
```

对于关闭的迭代器，池大小为1就足够了：

```ts
const sink = new CanvasSink(videoTrack, { poolSize: 1 });
const canvases = sink.canvases();

for await (const { canvas, timestamp } of canvases) {
	// ...
}
```

## 音频数据接收器

这些接收器只能与 `InputAudioTrack` 一起使用。

### `AudioSampleSink`

使用此接收器(sink)可以从音频轨道中提取解码后的[音频样本](./packets-and-samples#audiosample)。该接收器会自动处理内部解码过程。

创建接收器的方式如下：

```ts
import { AudioSampleSink } from 'mediabunny';

const sink = new AudioSampleSink(audioTrack);
```

获取样本的方法与`VideoSampleSink`上的方法类似：

* `getSample`\
  获取指定时间戳的样本；参见[单次获取](#single-retrieval)。
* `samples`\
  遍历一个范围内的样本；参见[范围遍历](#range-iteration)。
* `samplesAtTimestamps`\
  遍历特定时间戳的样本；参见[稀疏遍历](#sparse-iteration)。

这些方法会生成[`AudioSample`](./packets-and-samples#audiosample)实例。

例如，我们可以使用此接收器通过[均方根](https://en.wikipedia.org/wiki/Root_mean_square)计算音频轨道的平均响度：

```ts
const sink = new AudioSampleSink(audioTrack);

let sumOfSquares = 0;
let totalSampleCount = 0;

for await (const sample of sink.samples()) {
	const bytesNeeded = sample.allocationSize({ format: 'f32', planeIndex: 0 });
	const floats = new Float32Array(bytesNeeded / 4);
	sample.copyTo(floats, { format: 'f32', planeIndex: 0 });

	for (let i = 0; i < floats.length; i++) {
		sumOfSquares += floats[i] ** 2;
	}

	totalSampleCount += floats.length;
}

const averageLoudness = Math.sqrt(sumOfSquares / totalSampleCount);
```

### `AudioBufferSink`

虽然 `AudioSampleSink` 提取的是原始解码音频样本，但你可以使用 `AudioBufferSink` 直接提取 [`AudioBuffer`](https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer) 实例。这在处理 Web Audio API 时特别有用。

创建该接收器的方式如下：

```ts
import { AudioBufferSink } from 'mediabunny';

const sink = new AudioBufferSink(audioTrack);
```

获取音频缓冲区的方法与 `VideoSampleSink` 上的方法类似：

* `getBuffer`\
  获取指定时间戳的缓冲区；参见[单次检索](#single-retrieval)。
* `buffers`\
  遍历一个范围内的缓冲区；参见[范围迭代](#range-iteration)。
* `buffersAtTimestamps`\
  遍历特定时间戳的缓冲区；参见[稀疏迭代](#sparse-iteration)。

这些方法会生成 `WrappedAudioBuffer` 实例：

```ts
type WrappedAudioBuffer = {
	// 可用于 Web Audio API 的 AudioBuffer。
	buffer: AudioBuffer;
	// 对应音频样本的时间戳，单位为秒。
	timestamp: number;
	// 对应音频样本的持续时间，单位为秒。
	duration: number;
};
```

例如，让我们使用该接收器播放音频轨道的最后 10 秒：

```ts
const sink = new AudioBufferSink(audioTrack);
const audioContext = new AudioContext();
const lastTimestamp = await audioTrack.computeDuration();
const baseTime = audioContext.currentTime;

for await (const { buffer, timestamp } of sink.buffers(lastTimestamp - 10)) {
	const source = audioContext.createBufferSource();
	source.buffer = buffer;
	source.connect(audioContext.destination);
	source.start(baseTime + timestamp);
}
```

\[^1]: Almost: `CanvasSink` becomes stateful when using a [canvas pool](#canvas-pool).

---

---
url: /guide/converting-media-files.md
---
# 媒体文件转换

Mediabunny 中的[读取](./reading-media-files)和[写入](./writing-media-files)基础功能提供了转换媒体文件所需的一切。但由于这是非常常见的操作且细节可能比较复杂，Mediabunny 内置了一个文件转换抽象层。

它具有以下特性：

* 转封装（改变容器格式）
* 转码（改变轨道的编解码器）
* 轨道移除
* 压缩
* 剪辑
* 视频尺寸调整与适配
* 视频旋转
* 视频裁剪
* 视频帧率调整
* 音频重采样
* 音频上/下混音

该转换 API 设计简洁、功能多样且性能极高。

## 基本用法

### 运行转换

每个转换过程都由一个 `Conversion` 实例表示。使用 `Conversion.init(...)` 创建新实例，然后通过 `.execute()` 执行转换。

以下是将媒体转换为 WebM 格式的示例：

```ts
import {
	Input,
	Output,
	WebMOutputFormat,
	BufferTarget,
	Conversion,
} from 'mediabunny';

const input = new Input({ ... });
const output = new Output({
	format: new WebMOutputFormat(),
	target: new BufferTarget(),
});

const conversion = await Conversion.init({ input, output });
await conversion.execute();

// output.target.buffer 包含最终生成的文件
```

就是这么简单！`Conversion` 只需接收 `Input` 和 `Output` 实例，然后从输入读取数据并写入输出。如果您不熟悉 [`Input`](./reading-media-files) 和 [`Output`](./writing-media-files)，请查阅它们各自的指南。

::: info
传递给 `Conversion` 的 `Output` 必须是*全新*的，即不能包含已添加的轨道且处于 `'pending'` 状态（尚未开始）。
:::

在默认配置下，转换过程会自动处理所有细节，例如：

* 尽可能复制媒体数据，否则进行转码
* 丢弃输出格式不支持的轨道

在执行 `Conversion` 前，建议先检查[被丢弃的轨道](#discarded-tracks)。

### 监控转换进度

要监控 `Conversion` 的进度，请在调用 `execute` *之前* 设置其 `onProgress` 属性：

```ts
const conversion = await Conversion.init({ input, output });

conversion.onProgress = (progress: number) => {
	// `progress` 是一个介于 0 和 1 之间的数字（包含 0 和 1）
};

await conversion.execute();
```

每次转换进度更新时，都会调用此回调函数。

::: warning
进度值为 `1` 并不表示转换已完成；只有当 `.execute()` 返回的 Promise 解析时，转换才真正完成。
:::

::: warning
跟踪转换进度可能会轻微影响性能，因为这需要知道输入文件的总时长。通常影响可以忽略不计，但在使用仅追加输入源（如 [`ReadableStreamSource`](./reading-media-files#readablestreamsource)）时应避免使用此功能。
:::

如果你想监控转换输出的文件大小（以字节为单位），可以直接在 `Target` 上使用 `onwrite` 回调：

```ts
let currentFileSize = 0;

output.target.onwrite = (start, end) => {
	currentFileSize = Math.max(currentFileSize, end);
};
```

### 取消转换

有时你可能需要取消正在进行的转换过程。为此，可以使用 `cancel` 方法：

```ts
await conversion.cancel(); // 转换取消后解析
```

该方法会自动释放转换过程使用的所有资源。

## 视频选项

您可以在转换选项中设置 `video` 属性来配置转换器对视频轨道的处理行为。可用选项如下：

```ts
type ConversionVideoOptions = {
	discard?: boolean;
	width?: number;
	height?: number;
	fit?: 'fill' | 'contain' | 'cover';
	rotate?: 0 | 90 | 180 | 270;
	crop?: { left: number; top: number; width: number; height: number };
	frameRate?: number;
	codec?: VideoCodec;
	bitrate?: number | Quality;
	forceTranscode?: boolean;
};
```

例如，这里我们将视频轨道调整为720p分辨率：

```ts
const conversion = await Conversion.init({
	input,
	output,
	video: {
		width: 1280,
		height: 720,
		fit: 'contain',
	},
});
```

::: info
提供的配置将同等应用于输入的所有视频轨道。如果您想为每个视频轨道应用单独的配置，请查看[轨道特定选项](#track-specific-options)。
:::

### 丢弃视频

如果您想要移除视频轨道，请使用 `discard: true`。

### 视频尺寸调整

`width`、`height` 和 `fit` 属性控制视频的尺寸调整方式。如果只提供 `width` 或 `height`，另一个值会自动推算以保持视频原始宽高比。如果同时使用两者，必须设置 `fit` 来控制适配算法：

* `'fill'` 会拉伸图像以填满整个框，可能改变宽高比。
* `'contain'` 会在保持宽高比的情况下将整个图像包含在框内，可能导致黑边。
* `'cover'` 会缩放图像直到填满整个框，同时保持宽高比。

如果在 `rotation` 或 `crop` 的同时使用 `width` 或 `height`，它们将控制旋转后、裁剪后的尺寸。

若要对视频尺寸应用最大/最小约束，请查看 [轨道特定选项](#track-specific-options)。

在极少数情况下，如果输入视频的尺寸随时间变化，可以使用 `fit` 字段来控制尺寸变化行为（参见 [`VideoEncodingConfig`](./media-sources#video-encoding-config)）。未设置时，默认行为是 `'passThrough'`。

### 视频旋转

`rotation` 将视频按指定角度顺时针旋转。此旋转会叠加在原始输入文件中的任何旋转元数据之上，并在裁剪和尺寸调整之前应用。

### 视频裁剪

`crop` 可用于从原始视频中提取矩形区域。该矩形通过 `left`、`top`、`width` 和 `height` 指定，并会被限制在视频尺寸范围内。裁剪在旋转之后、尺寸调整之前应用。

### 调整帧率

`frameRate` 属性可用于设置输出视频的帧率（单位：Hz）。如果未指定，将使用原始输入帧率（可能是可变帧率）。

### 视频转码

使用 `codec` 属性来控制输出轨道的编解码器。这应该设置为输出文件支持的[编解码器](./supported-formats-and-codecs#video-codecs)，否则该轨道将被[丢弃](#discarded-tracks)。

使用 `bitrate` 属性来控制输出视频的比特率。例如，您可以使用此字段来压缩视频轨道。可接受的值为每秒比特数或[主观质量](./media-sources#subjective-qualities)。如果设置了此属性，将始终执行转码。如果未设置此属性但仍需要转码，则将使用 `QUALITY_HIGH` 作为默认值。

如果您希望阻止直接复制媒体数据并强制进行转码步骤，请使用 `forceTranscode: true`。

## 音频选项

您可以在转换选项中设置 `audio` 属性来配置转换器对音频轨道的行为。可用选项如下：

```ts
type ConversionAudioOptions = {
	discard?: boolean;
	codec?: AudioCodec;
	bitrate?: number | Quality;
	numberOfChannels?: number;
	sampleRate?: number;
	forceTranscode?: boolean;
};
```

例如，这里我们将音频轨道转换为单声道并设置特定的采样率：

```ts
const conversion = await Conversion.init({
	input,
	output,
	audio: {
		numberOfChannels: 1,
		sampleRate: 48000,
	},
});
```

::: info
提供的配置将同等应用于输入的所有音频轨道。如果您想为每个音频轨道应用单独的配置，请查看[轨道特定选项](#track-specific-options)。
:::

### 丢弃音频

如果您想去除音频轨道，请使用 `discard: true`。

### 音频重采样

`numberOfChannels` 属性控制输出音频的声道数（例如：1 表示单声道，2 表示立体声）。如果该值与输入音轨的声道数不同，Mediabunny 将使用[与 Web Audio API 相同的算法](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API#audio_channels)对声道数据进行上/下混音处理。

`sampleRate` 属性控制采样率（以 Hz 为单位，例如 44100、48000）。如果该值与输入音轨的采样率不同，Mediabunny 将对音频进行重采样。

### 音频转码

使用 `codec` 属性控制输出音轨的编解码器。该值应设置为输出文件支持的[编解码器](./supported-formats-and-codecs#audio-codecs)，否则音轨将被[丢弃](#discarded-tracks)。

使用 `bitrate` 属性控制输出音频的比特率。例如，您可以通过此字段压缩音频轨道。可接受的值包括每秒比特数或[主观质量等级](./media-sources#subjective-qualities)。如果设置了此属性，将始终执行转码。如果未设置此属性但仍需转码，则默认使用 `QUALITY_HIGH` 作为值。

如需阻止直接复制媒体数据并强制进行转码步骤，请使用 `forceTranscode: true`。

## 轨道特定选项

您可能希望根据输入轨道的具体情况来分别配置视频和音频选项。或者，当媒体文件包含多个视频或音频轨道时，您可能只想丢弃特定轨道或单独配置每个轨道。

为此，您可以为 `video` 和 `audio` 传递一个函数，而非直接传递对象：

```ts
const conversion = await Conversion.init({
	input,
	output,

	// 该函数会为每个视频轨道调用：
	video: (videoTrack, n) => {
		if (n > 1) {
			// 仅保留第一个视频轨道
			return { discard: true };
		}

		return {
			// 仅在轨道宽度大于640时才缩小宽度
			width: Math.min(videoTrack.displayWidth, 640),
		};
	},

	// 异步函数同样适用：
	audio: async (audioTrack, n) => {
		if (audioTrack.languageCode !== 'rus') {
			// 仅保留俄语音频轨道
			return { discard: true };
		}

		return {
			codec: 'aac',
		};
	},
});
```

关于视频和音频轨道属性的文档，请参阅[读取轨道元数据](./reading-media-files#reading-track-metadata)。

## 剪辑

在转换选项中使用 `trim` 属性可以从输入文件中提取指定片段到输出文件：

```ts
type ConversionOptions = {
	// ...
	trim?: {
		start: number; // 单位：秒
		end: number; // 单位：秒
	};
	// ...
};
```

例如，这里我们提取从10秒到25秒的片段：

```ts
const conversion = await Conversion.init({
	input,
	output,
	trim: {
		start: 10,
		end: 25,
	},
});
```

这种情况下，输出文件时长将为15秒。

如果仅设置 `start`，片段将从指定时间持续到输入文件结尾。如果仅设置 `end`，片段将从文件开头持续到指定时间。

## 元数据标签

默认情况下，输入文件的所有[描述性元数据标签](../api/MetadataTags.md)都会被复制到输出文件中。如果你想进一步控制写入输出的元数据标签，可以使用 `tags` 选项：

```ts
// 设置自定义元数据：
const conversion = await Conversion.init({
	// ...
	tags: () => ({
		title: 're:Turning',
		artist: 'Alexander Panos',
	}),
	// ...
});

// 或者，增强输入文件的元数据：
const conversion = await Conversion.init({
	// ...
	tags: inputTags => ({
		...inputTags, // 保留现有元数据
		images: [{ // 并添加封面图片
			data: new Uint8Array(...),
			mimeType: 'image/jpeg',
			kind: 'coverFront',
		}],
		comment: undefined, // 同时移除所有注释
	}),
	// ...
});

// 或者，移除所有元数据
const conversion = await Conversion.init({
	// ...
	tags: () => ({}),
	// ...
});
```

## 被丢弃的音轨

如果某个输入音轨未被包含在输出文件中，则被视为*被丢弃*。初始化 `Conversion` 后可以访问被丢弃音轨的列表：

```ts
const conversion = await Conversion.init({ input, output });
conversion.discardedTracks; // => DiscardedTrack[]

type DiscardedTrack = {
	// 被丢弃的音轨
	track: InputTrack;
	// 丢弃原因
	reason:
		| 'discarded_by_user' // 用户手动丢弃
		| 'max_track_count_reached' // 达到最大音轨数限制
		| 'max_track_count_of_type_reached' // 达到该类型音轨的最大数量限制
		| 'unknown_source_codec' // 未知的源编解码器
		| 'undecodable_source_codec' // 无法解码的源编解码器
		| 'no_encodable_target_codec'; // 无可编码的目标编解码器
};
```

由于在执行 `Conversion` 前可以检查此列表，这为你提供了决定是否继续转换流程的选择权。

***

存在以下原因：

* `discarded_by_user`\
  您通过设置 `discard: true` 丢弃了此轨道。
* `max_track_count_reached`\
  输出已没有空间容纳更多轨道。
* `max_track_count_of_type_reached`\
  输出已没有空间容纳该类型的更多轨道，或者输出根本不支持此轨道类型。
* `unknown_source_codec`\
  我们不知道输入轨道的编解码器，因此无法处理它。
* `undecodable_source_codec`\
  输入轨道的编解码器已知，但我们无法解码它。
* `no_encodable_target_codec`\
  我们找不到能够编码且能被输出格式包含的编解码器。如果环境不支持必要的编码器，或者您请求的编解码器无法被输出格式包含，则可能出现此原因。

***

另一方面，你也可以随时查询哪些输入音轨被成功包含在输出中：

```ts
const conversion = await Conversion.init({ input, output });
conversion.utilizedTracks; // => InputTrack[]
```

---

---
url: /guide/media-sources.md
---
# 媒体源

## 简介

*媒体源* 提供了向输出文件添加媒体数据的 API。不同的媒体源提供了不同层次的抽象，适用于不同的使用场景。

关于如何使用媒体源创建输出轨道的信息，请查看 [写入媒体文件](./writing-media-files)。

大多数媒体源遵循以下代码模式来添加媒体数据：

```ts
await mediaSource.add(...);
```

### 关闭媒体源

当你使用完媒体源（即不再需要添加更多媒体数据）时，最好尽快关闭它：

```ts
mediaSource.close();
```

从技术上讲，手动关闭媒体源并不是必须的，它会在最终化 `Output` 时自动关闭。然而，如果你的 `Output` 包含多个轨道，并且它们完成数据供给的时间不同（例如先添加所有音频再添加所有视频），提前关闭媒体源可以提高性能并降低内存使用。这是因为 `Output` 可以更好地"提前规划"，知道它不需要再等待某些轨道了（参见 [数据包缓冲](./writing-media-files#packet-buffering)）。因此，最佳实践是在使用完媒体源后立即手动关闭它们。

### 背压机制

媒体源（media sources）是将背压（backpressure）从输出管道传递到应用程序逻辑的媒介。当编码器或[StreamTarget](./writing-media-files#streamtarget)的可写流无法及时处理数据时，`Output`可能会施加背压。

背压通过Promise由媒体源进行通信。所有包含`add`方法的媒体源都会返回一个Promise：

```ts
mediaSource.add(...); // => Promise<void>
```

当媒体源准备好接收更多数据时，这个Promise会立即解决。但在输出管道某些部分处理过载的情况下，Promise会保持pending状态，直到输出准备好继续处理。因此，通过await这个Promise，你可以自动将背压传递到应用程序逻辑中：

```ts
// 错误示例： // [!code error]
while (notDone) { // [!code error]
	mediaSource.add(...); // [!code error]
} // [!code error]

// 正确示例：
while (notDone) {
	await mediaSource.add(...);
}
```

### 视频编码配置

所有内部处理编码的视频源都需要指定 `VideoEncodingConfig`，用于配置使用的编解码器参数：

```ts
type VideoEncodingConfig = {
	codec: VideoCodec;
	bitrate: number | Quality;
	bitrateMode?: 'constant' | 'variable';
	latencyMode?: 'quality' | 'realtime';
	keyFrameInterval?: number;
	fullCodecString?: string;
	hardwareAcceleration?: 'no-preference' | 'prefer-hardware' | 'prefer-software';
	scalabilityMode?: string;
	contentHint?: string;
	sizeChangeBehavior?: 'deny' | 'passThrough' | 'fill' | 'contain' | 'cover';

	onEncodedPacket?: (
		packet: EncodedPacket,
		meta: EncodedVideoChunkMetadata | undefined
	) => unknown;
	onEncoderConfig?: (
		config: VideoEncoderConfig
	) => unknown;
};
```

* `codec`: 用于编码的[视频编解码器](./supported-formats-and-codecs#video-codecs)
* `bitrate`: 目标比特率（每秒比特数）。也可以使用[主观质量](#subjective-qualities)参数
* `bitrateMode`: 控制使用恒定比特率(CBR)还是可变比特率(VBR)
* `latencyMode`: WebCodecs API 定义的延迟模式。浏览器默认为 `quality`。媒体流驱动的视频源会自动使用 `realtime` 设置
* `keyFrameInterval`: 相邻关键帧之间的最大间隔时间（秒）。默认为5秒。更频繁的关键帧可以改善搜索性能但会增加文件大小。使用多视频轨道时，所有轨道应设为相同值
* `fullCodecString`: 可选指定视频编码器使用的完整编解码器字符串（遵循[WebCodecs编解码器注册表](https://www.w3.org/TR/webcodecs-codec-registry/)）。例如使用AVC时可设置为`'avc1.42001f'`。注意字符串必须与`codec`字段指定的编解码器匹配。未设置时将自动生成
* `hardwareAcceleration`: 编解码器硬件加速方式的提示参数。建议保持`'no-preference'`
* `scalabilityMode`: 由[WebRTC-SVC](https://w3c.github.io/webrtc-svc/#scalabilitymodes*)定义的可扩展编码模式标识符
* `contentHint`: 由[mst-content-hint](https://w3c.github.io/mst-content-hint/#video-content-hints)定义的视频内容提示
* `sizeChangeBehavior`: 视频帧尺寸可能随时间变化。此字段控制尺寸变化时的处理行为。默认为`'deny'`
* `onEncodedPacket`: 每个数据包成功编码后的回调函数。可用于追踪编码进度
* `onEncoderConfig`: 当WebCodecs API创建内部编码器配置时调用。可用于检查完整编解码器字符串

### 音频编码配置

所有内部处理编码的音频源都需要指定一个 `AudioEncodingConfig`，用于设定要使用的编解码器配置：

```ts
type AudioEncodingConfig = {
	codec: AudioCodec;
	bitrate?: number | Quality;
	bitrateMode?: 'constant' | 'variable';
	fullCodecString?: string;

	onEncodedPacket?: (
		packet: EncodedPacket,
		meta: EncodedAudioChunkMetadata | undefined
	) => unknown;
	onEncoderConfig?: (
		config: AudioEncoderConfig
	) => unknown;
};
```

* `codec`: 用于编码的[音频编解码器](./supported-formats-and-codecs#audio-codecs)。对于未压缩的 PCM 编解码器可以省略。
* `bitrate`: 目标比特率（每秒比特数）。也可以使用[主观质量](#subjective-qualities)参数替代。
* `bitrateMode`: 用于控制恒定比特率(CBR)或可变比特率(VBR)。
* `fullCodecString`: 可选指定音频编码器使用的完整编解码器字符串，遵循 [WebCodecs 编解码器注册表](https://www.w3.org/TR/webcodecs-codec-registry/)规范。例如使用 AAC 时可设为 `'mp4a.40.2'`。注意编解码器字符串必须与 `codec` 指定的编解码器匹配。若不设置此字段，将自动生成编解码器字符串。
* `onEncodedPacket`: 每个成功编码的数据包都会触发此回调。可用于监测编码进度。
* `onEncoderConfig`: 当 WebCodecs API 创建内部编码器配置时触发。可用于获取完整的编解码器字符串信息。

### 主观质量等级

Mediabunny 提供了五种主观质量等级选项，作为手动指定比特率的替代方案。系统会根据编解码器和音轨信息（采样率等）自动计算对应的比特率。

```ts
import {
	QUALITY_VERY_LOW,
	QUALITY_LOW,
	QUALITY_MEDIUM,
	QUALITY_HIGH,
	QUALITY_VERY_HIGH,
} from 'mediabunny';
```

## 视频源

视频源为 `Output` 上的视频轨道提供数据。它们都继承自抽象类 `VideoSource`。

### `VideoSampleSource`

该视频源接收 [视频样本](./packets-and-samples#videosample)，进行编码后将编码数据传递给输出。

```ts
import { VideoSampleSource } from 'mediabunny';

const sampleSource = new VideoSampleSource({
	codec: 'avc',
	bitrate: 1e6,
});

await sampleSource.add(videoSample);
videoSample.close(); // 如果不再需要该样本

// 可选强制将样本编码为关键帧：
await sampleSource.add(videoSample, { keyFrame: true });
```

### `CanvasSource`

该视频源简化了常见模式：在渲染循环中重复更新单个画布，并将每帧添加到输出文件。

```ts
import { CanvasSource, QUALITY_MEDIUM } from 'mediabunny';

const canvasSource = new CanvasSource(canvasElement, {
	codec: 'av1',
	bitrate: QUALITY_MEDIUM,
});

await canvasSource.add(0.0, 0.1); // 时间戳，持续时间（秒）
await canvasSource.add(0.1, 0.1);
await canvasSource.add(0.2, 0.1);

// 可选强制将帧编码为关键帧：
await canvasSource.add(0.3, 0.1, { keyFrame: true });
```

### `MediaStreamVideoTrackSource`

这是一个用于 [Media Capture and Streams API](https://developer.mozilla.org/en-US/docs/Web/API/Media_Capture_and_Streams_API) 的源。如果您需要将实时视频源（如摄像头或屏幕录制）传输到输出文件，请使用此源。

```ts
import { MediaStreamVideoTrackSource } from 'mediabunny';

// 获取用户屏幕
const stream = await navigator.mediaDevices.getDisplayMedia({ video: true });
const videoTrack = stream.getVideoTracks()[0];

const videoTrackSource = new MediaStreamVideoTrackSource(videoTrack, {
	codec: 'vp9',
	bitrate: 1e7,
});

// 确保正确处理内部错误
videoTrackSource.errorPromise.catch((error) => ...);
```

此源无需额外方法调用；一旦在 `Output` 上调用 `start()`，数据将自动捕获并传输到输出文件。如果不再需要用户的媒体流，请确保在完成 `Output` 后对 `videoTrack` 调用 `stop()`。

::: info
如果此源是 `Output` 中唯一的 MediaStreamTrack 源，那么它添加的第一个视频样本将从时间戳 0 开始。如果有多个源，则所有轨道中最早的媒体样本将从时间戳 0 开始，所有轨道将彼此完美同步。
:::

::: warning
`MediaStreamVideoTrackSource` 的内部机制与常规代码流程分离，但仍可能抛出错误，因此请确保使用 `errorPromise` 处理任何错误并停止 `Output`。
:::

### `EncodedVideoPacketSource`

这是所有视频源中最基础的一种，可用于直接将[编码数据包](./packets-and-samples#encodedpacket)传输到输出端。该源要求您自行处理编码过程，这样您既可以手动使用 WebCodecs API，也可以接入自己的编码栈。此外，您还可以直接从其他媒体文件中读取编码数据包，从而跳过解码和重新编码视频数据的过程。

```ts
import { EncodedVideoPacketSource } from 'mediabunny';

// 必须指定编解码器名称：
const packetSource = new EncodedVideoPacketSource('vp9');

await packetSource.add(packet1);
await packetSource.add(packet2);
```

> \[!重要提示]
> 必须按照解码顺序添加数据包。

首次调用 `add` 方法时，需要提供额外的元数据，以便 `Output` 能获取更多关于视频数据形态的信息。这些元数据必须符合 WebCodecs API 的 `EncodedVideoChunkMetadata` 格式，示例如下：

```ts
await packetSource.add(firstPacket, {
	decoderConfig: {
		codec: 'vp09.00.31.08',
		codedWidth: 1280,
		codedHeight: 720,
		colorSpace: {
			primaries: 'bt709',
			transfer: 'iec61966-2-1',
			matrix: 'smpte170m',
			fullRange: false,
		},
		description: undefined,
	},
});
```

所有编解码器都必须提供 `codec`、`codedWidth` 和 `codedHeight` 字段，而某些编解码器还需要 `description` 字段。其他字段（如 `colorSpace`）则是可选的。[WebCodecs 编解码器注册表](https://www.w3.org/TR/webcodecs-codec-registry/)规定了每种视频编解码器的 `codec` 和 `description` 格式，使用时必须遵循这些规范。

#### B帧（双向预测帧）

某些视频编解码器使用*B帧*，这种帧需要同时依赖前一帧和后一帧才能解码。例如，您可能会遇到如下情况：

```md
帧1: 0.0秒, I帧(关键帧)
帧2: 0.1秒, B帧
帧3: 0.2秒, P帧
```

这些帧的解码顺序将是：

```md
帧1 -> 帧3 -> 帧2
```

某些文件格式通过明确的"解码时间戳"和"呈现时间戳"概念来建模B帧或乱序解码。然而，Mediabunny数据包只指定其*呈现时间戳*。解码顺序由您添加数据包的顺序决定，因此在我们的示例中，您必须按以下方式添加数据包：

```ts
await packetSource.add(packetForFrame1); // 0.0s
await packetSource.add(packetForFrame3); // 0.2s
await packetSource.add(packetForFrame2); // 0.1s
```

系统允许您提供呈现时间戳严重乱序的序列，但有一个硬性限制：

> \[!重要]
> 您添加的数据包的时间戳，不得小于在添加最后一个关键帧之前已添加的最大时间戳。

这个表述比较拗口，以下示例希望能阐明这一点：

```md

# 合法情况:
数据包1: 0.0秒, 关键帧
数据包2: 0.3秒, 差异帧
数据包3: 0.2秒, 差异帧
数据包4: 0.1秒, 差异帧
数据包5: 0.4秒, 关键帧
数据包6: 0.5秒, 差异帧


# 同样合法:
数据包1: 0.0秒, 关键帧
数据包2: 0.3秒, 差异帧
数据包3: point2秒, 差异帧
数据包4: 0.1秒, 差异帧
数据包5: 0.4秒, 关键帧
数据包6: 0.35秒, 差异帧
数据包7: 0.3秒, 差异帧
数据包8: 0.5秒, 差异帧


# 非法情况:
数据包1: 0.0秒, 关键帧
数据包2: 0.3秒, 差异帧
数据包3: 0.2秒, 差异帧
数据包4: 0.1秒, 差异帧
数据包5: 0.4秒, 关键帧
数据包6: 0.25秒, 差异帧
```

## 音频源

音频源向`Output`上的音频轨道提供数据。它们都继承自抽象类`AudioSource`。

### `AudioSampleSource`

该源接收[音频样本](./packets-and-samples#audiosample)，进行编码后，将编码数据传递至输出端。

```ts
import { AudioSampleSource } from 'mediabunny';

const sampleSource = new AudioSampleSource({
	codec: 'aac',
	bitrate: 128e3,
});

await sampleSource.add(audioSample);
audioSample.close(); // 如果不再需要该样本
```

### `AudioBufferSource`

该源直接接收 `AudioBuffer` 实例作为数据，简化了与 Web Audio API 的集成使用。首个 AudioBuffer 将在时间戳 0 处播放，后续的 AudioBuffer 会依次追加到之前所有 AudioBuffer 之后。

```ts
import { AudioBufferSource, QUALITY_MEDIUM } from 'mediabunny';

const bufferSource = new AudioBufferSource({
	codec: 'opus',
	bitrate: QUALITY_MEDIUM,
});

await bufferSource.add(audioBuffer1);
await bufferSource.add(audioBuffer2);
await bufferSource.add(audioBuffer3);
```

### `MediaStreamAudioTrackSource`

这是一个用于 [Media Capture and Streams API](https://developer.mozilla.org/en-US/docs/Web/API/Media_Capture_and_Streams_API) 的音频源。当您需要将实时音频源（如麦克风或用户计算机的音频）传输到输出文件时，可以使用此源。

```ts
import { MediaStreamAudioTrackSource } from 'mediabunny';

// 获取用户的麦克风
const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
const audioTrack = stream.getAudioTracks()[0];

const audioTrackSource = new MediaStreamAudioTrackSource(audioTrack, {
	codec: 'opus',
	bitrate: 128e3,
});

// 确保捕获内部错误并正确处理
audioTrackSource.errorPromise.catch((error) => ...);
```

此源无需额外的方法调用；一旦在 `Output` 上调用 `start()`，数据将自动捕获并传输到输出文件。如果您不再需要用户的媒体流，请在完成 `Output` 后记得在 `audioTrack` 上调用 `stop()`。

::: info
如果此源是 `Output` 中唯一的 MediaStreamTrack 源，那么它添加的第一个音频样本将从时间戳 0 开始。如果有多个源，则所有轨道中最早的媒体样本将从时间戳 0 开始，并且所有轨道将彼此完美同步。
:::

::: warning
`MediaStreamAudioTrackSource` 的内部机制与常规代码流程分离，但仍可能抛出错误，因此请确保使用 `errorPromise` 处理任何错误并停止 `Output`。
:::

### `EncodedAudioPacketSource`

这是所有音频源中最基础的一个，该源可用于直接将音频数据的[编码数据包](./packets-and-samples#encodedpacket)传输到输出端。此源需要您自行处理编码过程，使您能够手动使用 WebCodecs API 或接入自己的编码栈。或者，您也可以通过直接从其他媒体文件读取来获取编码数据包，从而跳过解码和重新编码音频数据的过程。

```ts
import { EncodedAudioPacketSource } from 'mediabunny';

// 必须指定编解码器名称：
const packetSource = new EncodedAudioPacketSource('aac');

await packetSource.add(packet);
```

您需要在首次调用 `add` 方法时提供额外的元数据，以便为 `Output` 提供有关音频数据形态的更多信息。这些元数据必须采用 WebCodecs API 的 `EncodedAudioChunkMetadata` 格式，可能如下所示：

```ts
await packetSource.add(firstPacket, {
	decoderConfig: {
		codec: 'mp4a.40.2',
		numberOfChannels: 2,
		sampleRate: 48000,
		description: new Uint8Array([17, 144]),
	},
});
```

对于所有编解码器，`codec`、`numberOfChannels` 和 `sampleRate` 都是必需的，而某些编解码器还需要 `description`。[WebCodecs 编解码器注册表](https://www.w3.org/TR/webcodecs-codec-registry/)规定了每个音频编解码器的 `codec` 和 `description` 格式，您必须遵守这些规范。

## 字幕源

字幕源为 `Output` 上的字幕轨道提供数据。它们都继承自抽象的 `SubtitleSource` 类。

### `TextSubtitleSource`

该源从定义字幕的文本文件中向输出提供字幕线索。

```ts
import { TextSubtitleSource } from 'mediabunny';

const textSource = new TextSubtitleSource('webvtt');

const text = 
`WEBVTT

00:00:00.000 --> 00:00:02.000
这是你最后的机会。

00:00:02.500 --> 00:00:04.000
此后，就没有回头路了。

00:00:04.500 --> 00:00:06.000
如果你选择蓝色药丸，故事就此结束。

00:00:06.500 --> 00:00:08.000
你会在床上醒来，相信任何你想相信的事。

00:00:08.500 --> 00:00:10.000	
如果你选择红色药丸，你将留在仙境

00:00:10.500 --> 00:00:12.000
我会让你看到兔子洞有多深。
`;

await textSource.add(text);
```

如果你一次性添加整个字幕文件，请确保在之后立即[关闭源](#closing-sources)：

```ts
textSource.close();
```

你也可以分小块逐个添加字幕线索：

```ts
import { TextSubtitleSource } from 'mediabunny';

const textSource = new TextSubtitleSource('webvtt');

await textSource.add('WEBVTT\n\n');
await textSource.add('00:00:00.000 --> 00:00:02.000\n你好！\n\n');
await textSource.add('00:00:02.500 --> 00:00:04.000\n分块内容。\n\n');
```

这些分块有一些限制条件：一个字幕线索必须完整包含在一个块中，不能拆分到多个小块中（尽管一个块可以包含多个线索）。此外，WebVTT 前言必须首先且一次性全部添加。

---

---
url: /guide/installation.md
---
# 安装

使用您喜欢的包管理器安装 Mediabunny：

::: code-group

```bash [npm]
npm install mediabunny
```

```bash [yarn]
yarn add mediabunny
```

```bash [pnpm]
pnpm add mediabunny
```

```bash [bun]
bun add mediabunny
```

:::

::: info
需要支持 ECMAScript 2021 或更高版本的 JavaScript 环境。Mediabunny 设计用于现代浏览器运行。如需类型支持，需要 TypeScript 5.7 或更高版本。
:::

然后，可以像这样简单导入：

```ts
import { ... } from 'mediabunny'; // ESM
const { ... } = require('mediabunny'); // 或 CommonJS
```

推荐使用 ESM 模块，因为它支持 tree shaking 优化。

您也可以直接在 HTML 中使用 script 标签引入库：

```html
<script src="mediabunny.cjs"></script>
```

这将在全局作用域中添加一个 `Mediabunny` 对象。您可以使用 `mediabunny.d.ts` 为这个全局对象提供类型声明。

您可以从 [发布页面](https://github.com/Vanilagy/mediabunny/releases) 下载构建好的分发文件。使用 `*.cjs` 构建文件用于普通 script 标签引入，或使用 `*.mjs` 构建文件用于 `type="module"` 的 script 标签或通过 ESM 直接导入。在 TypeScript 项目中包含 `mediabunny.d.ts` 声明文件将声明全局的 `Mediabunny` 命名空间。

---

---
url: /guide/quick-start.md
---
# 快速开始

本页收集了一些简短的代码片段，展示了您可能使用该库执行的最常见操作。

## 读取文件元数据

```ts
import { Input, ALL_FORMATS, BlobSource } from 'mediabunny';

const input = new Input({
	formats: ALL_FORMATS, // 支持所有文件格式
	source: new BlobSource(file), // 假设有一个 File 实例
});

const duration = await input.computeDuration(); // 单位为秒
const allTracks = await input.getTracks(); // 所有轨道的列表

// 提取视频元数据
const videoTrack = await input.getPrimaryVideoTrack();
if (videoTrack) {
	videoTrack.displayWidth; // 单位为像素
	videoTrack.displayHeight; // 单位为像素
	videoTrack.rotation; // 顺时针角度

	// 估算帧率 (FPS)
	const packetStats = await videoTrack.computePacketStats(100);
	const averageFrameRate = packetStats.averagePacketRate;
}

// 提取音频元数据
const audioTrack = await input.getPrimaryAudioTrack();
if (audioTrack) {
	audioTrack.numberOfChannels;
	audioTrack.sampleRate; // 单位为 Hz
}
```

::: info

* 查看元数据提取示例来了解这段代码的实际应用。
* 您不仅可以从 `File` 实例读取 - 更多选择请查看[输入源](./reading-media-files#input-sources)。
  :::

## 读取媒体数据

```ts
import {
	Input,
	ALL_FORMATS,
	BlobSource,
	VideoSampleSink,
	AudioSampleSink,
} from 'mediabunny';

const input = new Input({
	formats: ALL_FORMATS,
	source: new BlobSource(file),
});

// 读取视频帧
const videoTrack = await input.getPrimaryVideoTrack();
if (videoTrack) {
	const decodable = await videoTrack.canDecode();
	if (decodable) {
		const sink = new VideoSampleSink(videoTrack);

		// 获取5秒时间戳处的视频帧
		const videoSample = await sink.getSample(5);
		videoSample.timestamp; // 单位为秒
		videoSample.duration; // 单位为秒

		// 将帧绘制到画布上
		videoSample.draw(ctx, 0, 0);

		// 遍历视频前30秒的所有帧
		for await (const sample of sink.samples(0, 30)) {
			// ...
		}
	}
}

// 读取音频片段
const audioTrack = await input.getPrimaryAudioTrack();
if (audioTrack) {
	const decodable = await audioTrack.canDecode();
	if (decodable) {
		const sink = new AudioSampleSink(audioTrack);

		// 获取5秒时间戳处的音频片段；一小段音频
		const audioSample = await sink.getSample(5);
		audioSample.timestamp; // 单位为秒
		audioSample.duration; // 单位为秒
		audioSample.numberOfFrames;

		// 转换为AudioBuffer以便与Web Audio API一起使用
		const audioBuffer = audioSample.toAudioBuffer();

		// 遍历音频前30秒的所有片段
		for await (const sample of sink.samples(0, 30)) {
			// ...
		}
	}
}
```

::: info

* 查看媒体播放器示例了解基于此用例的演示。
* 参阅[媒体接收器](./media-sinks)了解从轨道提取媒体数据的所有方法。
  :::

## 提取视频缩略图

```ts
import {
	Input,
	ALL_FORMATS,
	BlobSource,
	CanvasSink,
} from 'mediabunny';

const input = new Input({
	formats: ALL_FORMATS,
	source: new BlobSource(file),
});

const videoTrack = await input.getPrimaryVideoTrack();
if (videoTrack) {
	const decodable = await videoTrack.canDecode();
	if (decodable) {
		const sink = new CanvasSink(videoTrack, {
			width: 320, // 自动调整缩略图尺寸
		});

		// 获取10秒时间点的缩略图
		const result = await sink.getCanvas(10);
		result.canvas; // HTMLCanvasElement | OffscreenCanvas
		result.timestamp; // 时间戳（秒）
		result.duration; // 持续时间（秒）

		// 在视频中生成五个等距分布的缩略图
		const startTimestamp = await videoTrack.getFirstTimestamp();
		const endTimestamp = await videoTrack.computeDuration();
		const timestamps = [0, 0.2, 0.4, 0.6, 0.8].map(
			(t) => startTimestamp + t * (endTimestamp - startTimestamp)
		);

		// 遍历这些时间点
		for await (const result of sink.canvasesAtTimestamps(timestamps)) {
			// ...
		}
	}
}
```

::: info

* 查看缩略图生成示例了解这段代码的实际应用。
* 你可以进一步配置[`CanvasSink`](./media-sinks#canvassink)。
  :::

## 提取编码数据包

```ts
import {
	Input,
	ALL_FORMATS,
	BlobSource,
	EncodedPacketSink,
} from 'mediabunny';

const input = new Input({
	formats: ALL_FORMATS,
	source: new BlobSource(file),
});

const videoTrack = await input.getPrimaryVideoTrack();
if (videoTrack) {
	const sink = new EncodedPacketSink(videoTrack);

	// 获取时间戳10秒处的数据包
	const packet = await sink.getPacket(10);
	packet.data; // Uint8Array
	packet.type; // 'key' | 'delta'
	packet.timestamp; // 单位秒
	packet.duration; // 单位秒

	// 获取最接近10秒时间戳的关键帧数据包
	const keyPacket = await sink.getKeyPacket(10);

	// 获取下一个数据包
	const nextPacket = await sink.getNextPacket(keyPacket);

	// 设置手动解码器
	const decoderConfig = await videoTrack.getDecoderConfig();
	const videoDecoder = new VideoDecoder({
		output: console.log,
		error: console.error,
	});
	videoDecoder.configure(decoderConfig);

	// 按解码顺序遍历所有数据包
	for await (const packet of sink.packets()) {
		videoDecoder.decode(packet.toEncodedVideoChunk());
	}

	await videoDecoder.flush();
}
```

::: info
查看 [`EncodedPacketSink`](./media-sinks#encodedpacketsink) 获取完整文档。
:::

## 创建新的媒体文件

```ts
import {
	Output,
	BufferTarget,
	Mp4OutputFormat,
	CanvasSource,
	AudioBufferSource,
	QUALITY_HIGH,
} from 'mediabunny';

// Output 代表一个新的媒体文件
const output = new Output({
	format: new Mp4OutputFormat(), // 文件格式
	target: new BufferTarget(), // 写入目标（这里是内存）
});

// 示例：添加由 canvas 驱动的视频轨道
const videoSource = new CanvasSource(canvas, {
	codec: 'avc',
	bitrate: QUALITY_HIGH,
});
output.addVideoTrack(videoSource);

// 示例：添加由 AudioBuffers 驱动的音频轨道
const audioSource = new AudioBufferSource({
	codec: 'aac',
	bitrate: QUALITY_HIGH,
});
output.addAudioTrack(audioSource);

await output.start();

// 添加视频帧
for (let frame = 0; ...) {
	await videoSource.add(frame / 30, 1 / 30);
}

// 添加音频数据
await audioSource.add(audioBuffer1);
await audioSource.add(audioBuffer2);

await output.finalize();

const buffer = output.target.buffer; // 包含最终 MP4 文件的 ArrayBuffer
```

::: info

* 查看程序化生成示例了解浏览器内视频生成的演示。
* 您可以创建多种不同格式的文件，查看[输出格式](./output-formats)获取完整列表。
* 媒体数据可以从不同来源添加，参见[媒体源](./media-sources)。
  :::

## 直接写入磁盘

```ts
import {
	Output,
	StreamTarget,
} from 'mediabunny';

// 文件系统 API
const handle = await window.showSaveFilePicker();
const writableStream = await handle.createWritable();

const output = new Output({
	// `chunked: true` 用于批量磁盘操作
	target: new StreamTarget(writableStream, { chunked: true }),
	// ...
});

// ...

await output.finalize();

// 文件已完全写入磁盘
```

## 网络流传输

```ts
import {
	Output,
	StreamTarget,
	StreamTargetChunk,
	Mp4OutputFormat,
} from 'mediabunny';

const { writable, readable } = new TransformStream<StreamTargetChunk, Uint8Array>({
	transform: (chunk, controller) => controller.enqueue(chunk.data),
});

const output = new Output({
	target: new StreamTarget(writable),
	// 这里必须使用仅追加格式，例如分片MP4
	format: new Mp4OutputFormat({ fastStart: 'fragmented' }),
});

const uploadComplete = fetch('https://example.com/upload', {
	method: 'POST',
	body: readable,
	duplex: 'half',
	headers: {
		'Content-Type': output.format.mimeType,
	},
});

await output.start();

// ...

await output.finalize();
await uploadComplete;
```

::: info

* 这段代码会自动处理网络传输慢导致的背压问题
* 了解更多关于[仅追加格式](./output-formats#append-only-writing)的信息，这是此模式必需的条件
  :::

## 录制实时媒体

```ts
import {
	Output,
	BufferTarget,
	WebMOutputFormat,
	MediaStreamVideoTrackSource,
	MediaStreamAudioTrackSource,
	QUALITY_MEDIUM
} from 'mediabunny';

const userMedia = await navigator.mediaDevices.getUserMedia({
	video: true,
	audio: true,
});
const videoTrack = userMedia.getVideoTracks()[0];
const audioTrack = userMedia.getAudioTracks()[0];

const output = new Output({
	format: new WebMOutputFormat(),
	target: new BufferTarget(),
});

if (videoTrack) {
	const source = new MediaStreamVideoTrackSource(videoTrack, {
		codec: 'vp9',
		bitrate: QUALITY_MEDIUM,
	});
	output.addVideoTrack(source);
}

if (audioTrack) {
	const source = new MediaStreamAudioTrackSource(audioTrack, {
		codec: 'opus',
		bitrate: QUALITY_MEDIUM,
	});
	output.addAudioTrack(source);
}

await output.start();

// 等待...

await output.finalize();
```

::: info

* 查看实时录制演示了解这段代码的实际运行效果
* 这本质上是一个更好用的 [`MediaRecorder`](https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder) 替代方案
  :::

## 检查编码支持

```ts
import {
	MovOutputFormat,
	getFirstEncodableVideoCodec,
	getFirstEncodableAudioCodec,
	getEncodableVideoCodecs,
	getEncodableAudioCodecs,
} from 'mediabunny';

const outputFormat = new MovOutputFormat();

// 为指定容器格式寻找最佳支持的编解码器
const bestVideoCodec = await getFirstEncodableVideoCodec(
	outputFormat.getSupportedVideoCodecs(),
	// 可选参数，限制编码参数:
	{ width: 1920, height: 1080 },
);
const bestAudioCodec = await getFirstEncodableAudioCodec(
	outputFormat.getSupportedAudioCodecs(),
);

// 获取所有支持的编解码器
const supportedVideoCodecs = await getEncodableVideoCodecs();
const supportedAudioCodecs = await getEncodableAudioCodecs();
```

## 文件转换

```ts
import {
	Input,
	Output,
	Conversion,
	ALL_FORMATS,
	BlobSource,
	Mp4OutputFormat,
} from 'mediabunny';

// 更多 Input 和 Output 的示例请参考上面的代码片段
const input = new Input({
	formats: ALL_FORMATS,
	source: new BlobSource(file),
});
const output = new Output({
	format: new Mp4OutputFormat(),
	target: new BufferTarget(),
});

const conversion = await Conversion.init({ input, output });
conversion.discardedTracks; // 不会包含在输出中的轨道列表

conversion.onProgress = (progress) => {
	progress; // 0到1之间的进度值（包含0和1）
};

await conversion.execute();
// 转换完成

const buffer = output.target.buffer; // 包含最终MP4文件的ArrayBuffer
```

::: info

* 这段代码会在可能时自动进行转封装（直接复制媒体数据），必要时进行转码（重新编码媒体数据）。
* 完整文档请参考[转换媒体文件](./converting-media-files)。
  :::

## 提取音频

```ts
import {
	Input,
	Output,
	Conversion,
	WavOutputFormat,
} from 'mediabunny';

const input = new Input(...);
const output = new Output({
	// 输出为.wav文件，仅保留音频轨道
	format: new WavOutputFormat(),
	// ...
});

const conversion = await Conversion.init({
	input,
	output,
	audio: {
		sampleRate: 16000, // 重采样至16 kHz
	},
});
await conversion.execute();
// 转换完成
```

::: info

* 你也可以提取为其他纯音频格式，如.mp3、.ogg甚至.m4a。详见[输出格式](./output-formats)。
  :::

## 媒体压缩

```ts
import {
	Input,
	Output,
	Conversion,
	QUALITY_LOW,
} from 'mediabunny';

const input = new Input(...);
const output = new Output(...);

const conversion = await Conversion.init({
	input,
	output,
	video: {
		width: 480,
		bitrate: QUALITY_LOW,
	},
	audio: {
		numberOfChannels: 1,
		bitrate: QUALITY_LOW,
	},
	trim: {
		// 只保留前60秒的内容
		start: 0,
		end: 60,
	},
});

await conversion.execute();
// 转换完成
```

::: info

* 查看文件压缩示例来了解这段代码的实际应用。
  :::

---

---
url: /guide/supported-formats-and-codecs.md
---
# 支持的格式与编解码器

## 容器格式

Mediabunny 支持多种常用的媒体容器格式，所有格式都支持双向操作（读取和写入）：

* 基于 ISOBMFF 的格式 (.mp4, .m4v, .m4a, ...)
* QuickTime 文件格式 (.mov)
* Matroska (.mkv)
* WebM (.webm)
* Ogg (.ogg)
* MP3 (.mp3)
* WAVE (.wav)
* ADTS (.aac)

## 编解码器

Mediabunny 支持广泛的视频、音频和字幕编解码器。具体来说，它开箱即用地支持 WebCodecs API 指定的所有编解码器以及一些额外的 PCM 编解码器。

WebCodecs API 提供的编解码器可用性取决于浏览器，因此本库无法保证所有编解码器都能使用。Mediabunny 提供了[特殊工具函数](#querying-codec-encodability)来检查哪些编解码器可以被编码。如果浏览器不原生支持某个编解码器，您还可以指定[自定义编码器](#custom-coders)来提供自己的编码器/解码器实现。

::: info
Mediabunny 内置了所有音频 PCM 编解码器的解码器和编码器，这意味着这些编解码器始终可用。
:::

### 视频编解码器

* `'avc'` - 高级视频编码 (AVC) / H.264
* `'hevc'` - 高效视频编码 (HEVC) / H.265
* `'vp8'` - VP8
* `'vp9'` - VP9
* `'av1'` - AOMedia 视频 1 (AV1)

### 音频编解码器

* `'aac'` - 高级音频编码 (AAC)
* `'opus'` - Opus 音频编码
* `'mp3'` - MP3 音频格式
* `'vorbis'` - Vorbis 音频编码
* `'flac'` - 无损音频编解码器 (FLAC)
* `'pcm-u8'` - 8位无符号PCM
* `'pcm-s8'` - 8位有符号PCM
* `'pcm-s16'` - 16位小端有符号PCM
* `'pcm-s16be'` - 16位大端有符号PCM
* `'pcm-s24'` - 24位小端有符号PCM
* `'pcm-s24be'` - 24位大端有符号PCM
* `'pcm-s32'` - 32位小端有符号PCM
* `'pcm-s32be'` - 32位大端有符号PCM
* `'pcm-f32'` - 32位小端浮点PCM
* `'pcm-f32be'` - 32位大端浮点PCM
* `'pcm-f64'` - 64位小端浮点PCM
* `'pcm-f64be'` - 64位大端浮点PCM
* `'ulaw'` - μ-law PCM
* `'alaw'` - A-law PCM

### 字幕编解码器

* `'webvtt'` - WebVTT 字幕格式

## 兼容性表格

并非所有编解码器都能与所有容器格式配合使用。下表列出了支持的编解码器-容器组合：

|                |   .mp4   | .mov  | .mkv  | .webm\[^1] | .ogg  | .mp3  | .wav  | .aac  |
|:--------------:|:--------:|:-----:|:-----:|:---------:|:-----:|:-----:|:-----:|:-----:|
| `'avc'`        |    ✓     |   ✓   |   ✓   |           |       |       |       |       |
| `'hevc'`       |    ✓     |   ✓   |   ✓   |           |       |       |       |       |
| `'vp8'`        |    ✓     |   ✓   |   ✓   |     ✓     |       |       |       |       |
| `'vp9'`        |    ✓     |   ✓   |   ✓   |     ✓     |       |       |       |       |
| `'av1'`        |    ✓     |   ✓   |   ✓   |     ✓     |       |       |       |       |
| `'aac'`        |    ✓     |   ✓   |   ✓   |           |       |       |       |   ✓   |
| `'opus'`       |    ✓     |   ✓   |   ✓   |     ✓     |   ✓   |       |       |       |
| `'mp3'`        |    ✓     |   ✓   |   ✓   |           |       |   ✓   |       |       |
| `'vorbis'`     |    ✓     |   ✓   |   ✓   |     ✓     |   ✓   |       |       |       |
| `'flac'`       |    ✓     |   ✓   |   ✓   |           |       |       |       |       |
| `'pcm-u8'`     |          |   ✓   |   ✓   |           |       |       |   ✓   |       |
| `'pcm-s8'`     |          |   ✓   |       |           |       |       |       |       |
| `'pcm-s16'`    |    ✓     |   ✓   |   ✓   |           |       |       |   ✓   |       |
| `'pcm-s16be'`  |    ✓     |   ✓   |   ✓   |           |       |       |       |       |
| `'pcm-s24'`    |    ✓     |   ✓   |   ✓   |           |       |       |   ✓   |       |
| `'pcm-s24be'`  |    ✓     |   ✓   |   ✓   |           |       |       |       |       |
| `'pcm-s32'`    |    ✓     |   ✓   |   ✓   |           |       |       |   ✓   |       |
| `'pcm-s32be'`  |    ✓     |   ✓   |   ✓   |           |       |       |       |       |
| `'pcm-f32'`    |    ✓     |   ✓   |   ✓   |           |       |       |   ✓   |       |
| `'pcm-f32be'`  |    ✓     |   ✓   |       |           |       |       |       |       |
| `'pcm-f64'`    |    ✓     |   ✓   |   ✓   |           |       |       |       |       |
| `'pcm-f64be'`  |    ✓     |   ✓   |       |           |       |       |       |       |
| `'ulaw'`       |          |   ✓   |       |           |       |       |   ✓   |       |
| `'alaw'`       |          |   ✓   |       |           |       |       |   ✓   |       |
| `'webvtt'`\[^2] |   (✓)    |       |  (✓)  |    (✓)    |       |       |       |       |

## 查询编解码器编码能力

Mediabunny 提供了一些实用函数，可用于检查浏览器是否能够编码指定的编解码器。此外，您还可以检查编解码器是否支持特定的\_配置\_进行编码。

`canEncode` 测试编解码器是否可以使用典型设置进行编码：

```ts
import { canEncode } from 'mediabunny';

canEncode('avc'); // => Promise<boolean>
canEncode('opus'); // => Promise<boolean>
```

视频编解码器检查使用 1280x720 @1Mbps 的配置，而音频编解码器检查使用 2 声道、48 kHz @128kbps 的配置。

您也可以使用特定配置检查编码能力：

```ts
import { canEncodeVideo, canEncodeAudio } from 'mediabunny';

canEncodeVideo('hevc', {
	width: 1920, height: 1080, bitrate: 1e7
}); // => Promise<boolean>

canEncodeAudio('aac', {
	numberOfChannels: 1, sampleRate: 44100, bitrate: 192e3
}); // => Promise<boolean>
```

此外，[`VideoEncodingConfig`](./media-sources#video-encoding-config) 和 [`AudioEncodingConfig`](./media-sources#audio-encoding-config) 的大部分属性也可以在此处使用。

***

此外，您可以使用以下函数一次性检查多个编解码器的编码能力，返回支持的编解码器列表：

```ts
import {
	getEncodableCodecs,
	getEncodableVideoCodecs,
	getEncodableAudioCodecs,
	getEncodableSubtitleCodecs,
} from 'mediabunny';

getEncodableCodecs(); // => Promise<MediaCodec[]>
getEncodableVideoCodecs(); // => Promise<VideoCodec[]>
getEncodableAudioCodecs(); // => Promise<AudioCodec[]>
getEncodableSubtitleCodecs(); // => Promise<SubtitleCodec[]>

// 这些函数也接受可选的配置选项
// 这里我们检查 AVC、HEVC 和 VP8 哪些可以在 1920x1080 @10Mbps 下编码：
getEncodableVideoCodecs(
	['avc', 'hevc', 'vp8'],
	{ width: 1920, height: 1080, bitrate: 1e7 },
); // => Promise<VideoCodec[]>
```

***

如果您只是想找到浏览器能够编码的最佳编解码器，可以使用以下函数，它们会返回浏览器支持的第一个编解码器：

```ts
import {
	getFirstEncodableVideoCodec,
	getFirstEncodableAudioCodec,
	getFirstEncodableSubtitleCodec,
} from 'mediabunny';

getFirstEncodableVideoCodec(['avc', 'vp9', 'av1']); // => Promise<VideoCodec | null>
getFirstEncodableAudioCodec(['opus', 'aac']); // => Promise<AudioCodec | null>

getFirstEncodableVideoCodec(
	['avc', 'hevc', 'vp8'],
	{ width: 1920, height: 1080, bitrate: 1e7 },
); // => Promise<VideoCodec | null>
```

如果列出的编解码器都不支持，则返回 `null`。

这些函数特别适合与[输出格式](./output-formats)结合使用，以获取编码器和容器格式都支持的最佳编解码器：

```ts
import {
	Mp4OutputFormat,
	getFirstEncodableVideoCodec,
} from 'mediabunny';

const outputFormat = new Mp4OutputFormat();
const containableVideoCodecs = outputFormat.getSupportedVideoCodecs();
const bestVideoCodec = await getFirstEncodableVideoCodec(containableVideoCodecs);
```

::: info
编解码器编码能力检查会考虑[自定义编码器](#custom-encoders)。
:::

## 查询编解码器可解码性

一个编解码器能否被解码取决于 `InputTrack` 的具体编解码配置；你可以使用它的 [`canDecode`](./reading-media-files#codec-information) 方法来检查。

## 自定义编解码器

Mediabunny 允许你注册自定义的编码器和解码器 - 这在你想为某些浏览器不支持的编解码器提供 polyfill，或者想在非 WebCodecs 环境（如 Node.js）中使用 Mediabunny 时非常有用。

编码器和解码器可以为库支持的[所有视频和音频编解码器](#codecs)进行注册。无法添加新的编解码器。

::: warning
Mediabunny 要求自定义编码器和解码器遵循非常具体的实现规则。请特别注意标有"**必须**"的部分以确保兼容性。
:::

### 自定义编码器

要创建自定义的视频或音频编码器，您需要创建一个继承 `CustomVideoEncoder` 或 `CustomAudioEncoder` 的类。然后，您**必须**使用 `registerEncoder` 注册这个类：

```ts
import { CustomAudioEncoder, registerEncoder } from 'mediabunny';

class MyAwesomeMp3Encoder extends CustomAudioEncoder {
	// ...
}
registerEncoder(MyAwesomeMp3Encoder);
```

每个编码器实例上都有以下属性，这些属性由库设置：

```ts
class {
	// 视频编码器：
	codec: VideoCodec;
	config: VideoEncoderConfig;
	onPacket: (packet: EncodedPacket, meta?: EncodedVideoChunkMetadata) => unknown;

	// 音频编码器：
	codec: AudioCodec;
	config: AudioEncoderConfig;
	onPacket: (packet: EncodedPacket, meta?: EncodedAudioChunkMetadata) => unknown;
}
```

`codec` 和 `config` 指定要使用的具体编解码器配置，`onPacket` 是一个方法，您的代码**必须**为每个创建的编码数据包调用该方法。

您**必须**在自定义编码器类中实现以下方法：

```ts
class {
	// 视频编码器：
	static supports(codec: VideoCodec, config: VideoEncoderConfig): boolean;
	// 音频编码器：
	static supports(codec: AudioCodec, config: AudioEncoderConfig): boolean;

	init(): Promise<void> | void;
	encode(sample: VideoSample, options: VideoEncoderEncodeOptions): Promise<void> | void; // 视频
	encode(sample: AudioSample): Promise<void> | void; // 音频
	flush(): Promise<void> | void;
	close(): Promise<void> | void;
}
```

* `supports`\
  这是一个*静态*方法，如果编码器能够编码指定的编解码器，则**必须**返回 `true`，否则返回 `false`。如果返回 `true`，库将创建您的编码器类的新实例并用于编码，优先于默认编码器。
* `init`\
  在类实例化后由库调用。在此处放置任何初始化逻辑。
* `encode`\
  为每个要编码的样本调用。然后将生成的编码数据包**必须**传递给 `onPacket` 方法。
* `flush`\
  当编码器需要完成所有尚未完成编码的剩余样本的编码过程时调用。此方法**必须**仅在传递给 `encode` 的所有样本都完全编码后才返回/解析。然后它**必须**重置其内部状态，为下一批编码做好准备。
* `close`\
  当不再需要编码器并可以释放其内部资源时调用。

::: info
类的所有实例方法都可以返回 Promise。在这种情况下，库将确保*序列化*所有方法调用，使得任何时候都不会有两个方法并发运行。
:::

::: warning
传递给 `onPacket` 的数据包**必须**按照[解码顺序](./media-sinks.md#decode-vs-presentation-order)排列。
:::

### 自定义解码器

要创建自定义视频或音频解码器，您需要创建一个继承 `CustomVideoDecoder` 或 `CustomAudioDecoder` 的类。然后，您**必须**使用 `registerDecoder` 注册这个类：

```ts
import { CustomAudioDecoder, registerDecoder } from 'mediabunny';

class MyAwesomeMp3Decoder extends CustomAudioDecoder {
	// ...
}
registerDecoder(MyAwesomeMp3Decoder);
```

每个解码器实例上都有以下属性，这些属性由库设置：

```ts
class {
	// 视频解码器专用:
	codec: VideoCodec;
	config: VideoDecoderConfig;
	onSample: (sample: VideoSample) => unknown;

	// 音频解码器专用:
	codec: AudioCodec;
	config: AudioDecoderConfig;
	onSample: (sample: AudioSample) => unknown;
}
```

`codec` 和 `config` 指定要使用的具体编解码器配置，`onSample` 是一个方法，您的代码**必须**为每个生成的视频/音频样本调用它。

您**必须**在自定义解码器类中实现以下方法：

```ts
class {
	// 视频解码器专用:
	static supports(codec: VideoCodec, config: VideoDecoderConfig): boolean;
	// 音频解码器专用:
	static supports(codec: AudioCodec, config: AudioDecoderConfig): boolean;

	init(): Promise<void> | void;
	decode(packet: EncodedPacket): Promise<void> | void;
	flush(): Promise<void> | void;
	close(): Promise<void> | void;
}
```

* `supports`\
  这是一个*静态*方法，如果解码器能够解码指定的编解码器则**必须**返回 `true`，否则返回 `false`。如果返回 `true`，库将创建您的解码器类的新实例并用于解码，优先级高于默认解码器。
* `init`\
  在类实例化后由库调用。在此放置任何初始化逻辑。
* `decode`\
  对每个要解码的 `EncodedPacket` 调用。解码后的视频或音频样本**必须**随后传递给 `onSample` 方法。
* `flush`\
  当需要解码器完成所有剩余未完成解码的数据包时调用。此方法**必须**仅在传递给 `decode` 的所有数据包完全解码后才返回/解决。然后**必须**重置其内部状态以准备下一批解码。
* `close`\
  当不再需要解码器时可以释放其内部资源时调用。

::: info
类的所有实例方法都可以返回 Promise。在这种情况下，库将确保*序列化*所有方法调用，使得没有两个方法会并发运行。
:::

::: warning
传递给 `onSample` 的样本**必须**按时间戳递增排序。这意味着如果解码器正在解码使用 [B帧](./media-sources.md#b-frames) 的视频流，解码器**必须**在内部保留这些帧，以便能按呈现时间戳排序输出。每次调用 `flush` 时，这个严格的排序要求会被重置。
:::

\[^1]: WebM only supports a small subset of the codecs supported by Matroska. However, this library can technically read all codecs from a WebM that are supported by Matroska.
\[^2]: WebVTT can only be written, not read.

---

---
url: /guide/packets-and-samples.md
---
# 数据包与采样

## 简介

Mediabunny 中的媒体数据以两种不同形式存在：

* **数据包(Packet):** 经过编码的媒体数据，编码过程的产物
* **采样(Sample):** 原始的、未压缩的、可直接呈现的媒体数据

除了数据本身，数据包和采样都携带额外的元数据，如时间戳、时长、宽度等。

数据包由 `EncodedPacket` 类表示，该类同时用于视频和音频数据包。采样则由 `VideoSample` 和 `AudioSample` 类表示：

* `VideoSample`: 表示单帧视频
* `AudioSample`: 表示一段(通常较短的)音频

采样可以被编码为数据包，数据包也可以解码为采样：

```mermaid
flowchart LR
	A[VideoSample]
	B[AudioSample]
	C[EncodedPacket]
	D[VideoSample]
	E[AudioSample]

	A -- 编码 --> C
	B -- 编码 --> C
	C -- 解码 --> D
	C -- 解码 --> E
```

### 与 WebCodecs 的关联

Mediabunny 中的数据包和采样直接对应 [WebCodecs API](https://w3c.github.io/webcodecs/) 中的概念：

* `EncodedPacket`\
  -> 视频数据包对应 `EncodedVideoChunk`\
  -> 音频数据包对应 `EncodedAudioChunk`
* `VideoSample`
  -> `VideoFrame`
* `AudioSample`
  -> `AudioData`

由于 Mediabunny 大量使用了 WebCodecs API，其自身的类通常作为 WebCodecs 类的包装器。但这种包装带来了几个优势：

1. **独立性：** 即使 WebCodecs API 不可用，该库仍能保持功能。编码器和解码器可以通过[自定义编解码器](./supported-formats-and-codecs#custom-coders)进行填充，且库能在非浏览器环境（如 Node.js）中运行。
2. **可扩展性：** 包装器作为附加操作的命名空间，例如 `AudioSample` 上的 `toAudioBuffer()`，或 `VideoSample` 上的 `draw()`。
3. **一致性：** WebCodecs 使用整数微秒时间戳，而 Mediabunny 统一使用浮点秒时间戳。通过这些包装器，所有时间信息始终以秒为单位，用户无需考虑单位转换。

转换操作很简单：

```ts
import { EncodedPacket, VideoSample, AudioSample } from 'mediabunny';

// EncodedPacket 转 WebCodecs chunks:
encodedPacket.toEncodedVideoChunk(); // => EncodedVideoChunk
encodedPacket.toEncodedAudioChunk(); // => EncodedAudioChunk
// WebCodecs chunks 转 EncodedPacket:
EncodedPacket.fromEncodedChunk(videoChunk); // => EncodedPacket
EncodedPacket.fromEncodedChunk(audioChunk); // => EncodedPacket

// VideoSample 转 VideoFrame:
videoSample.toVideoFrame(); // => VideoFrame
// VideoFrame 转 VideoSample:
new VideoSample(videoFrame); // => VideoSample

// AudioSample 转 AudioData:
audioSample.toAudioData(); // => AudioData
// AudioData 转 AudioSample:
new AudioSample(audioData); // => AudioSample
```

::: info
从 WebCodecs API 对应实例创建的 `VideoSample`/`AudioSample` 非常高效；它们仅保留对底层 WebCodecs API 实例的引用，不会执行任何不必要的复制操作。
:::

### 负时间戳

虽然数据包和样本的持续时间不能为负值，但数据包和样本的时间戳可以是负值。

负时间戳表示该样本在合成开始之前就开始播放（合成始终从0开始）。负时间戳通常是轨道在起始处被修剪的结果，可能是为了截断部分媒体内容或与其他轨道同步。因此，您应当避免呈现任何具有负时间戳的样本。

## `EncodedPacket`

编码数据包（EncodedPacket）代表任何类型（视频或音频）的已编码媒体数据。它们是*编码过程*的产物，您可以通过*解码过程*将编码数据包转换为实际的媒体数据。

### 创建数据包

要创建 `EncodedPacket`，可以使用其构造函数：

```ts
constructor(
    data: Uint8Array,
    type: 'key' | 'delta',
    timestamp: number, // 单位为秒
    duration: number, // 单位为秒
    sequenceNumber?: number,
    byteLength?: number,
);
```

::: info
通常情况下您不需要在构造函数中设置 `sequenceNumber` 或 `byteLength`。
:::

例如，这里我们从一些编码视频数据创建数据包：

```ts
import { EncodedPacket } from 'mediabunny';

const encodedVideoData = new Uint8Array([...]);
const encodedPacket = new EncodedPacket(encodedVideoData, 'key', 5, 1/24);
```

另外，如果您使用的是 WebCodecs 的编码块（encoded chunks），也可以从中创建 `EncodedPacket`：

```ts
import { EncodedPacket } from 'mediabunny';

// 从 EncodedVideoChunk 创建：
const encodedPacket = EncodedPacket.fromEncodedChunk(encodedVideoChunk);

// 从 EncodedAudioChunk 创建：
const encodedPacket = EncodedPacket.fromEncodedChunk(encodedAudioChunk);
```

### 数据包检查

编码后的数据包包含一系列只读数据可供检查。你可以这样获取编码数据：

```ts
encodedPacket.data; // => Uint8Array
```

你可以查询数据包类型：

```ts
encodedPacket.type; // => PacketType ('key' | 'delta')
```

* *关键帧数据包*（key packet）可以直接解码，独立于其他数据包。
* *增量数据包*（delta packet）必须在前一个数据包解码完成后才能解码。

例如，在视频轨道中，通常每隔几秒会有一个关键帧。当进行跳转时，如果用户跳转到关键帧之后不久的位置，可以快速显示解码后的数据；如果他们跳转到远离关键帧的位置，解码器必须先处理许多增量帧才能显示内容。

#### 确定数据包的实际类型

`type` 字段来源于包含文件的元数据，在极少数情况下可能不准确。要确切确定数据包的实际类型，可以这样做：

```ts
// `packet` 必须来自 InputTrack `track`
const type = await track.determinePacketType(packet); // => PacketType | null
```

该方法通过检查数据包的比特流来确定其类型。当无法确定类型时返回 `null`。

***

你可以查询数据包的时间信息：

```ts
encodedPacket.timestamp; // => 以秒为单位的呈现时间戳
encodedPacket.duration; // => 以秒为单位的持续时间

// 这些属性还有微秒级别的整数版本：
encodedPacket.microsecondTimestamp;
encodedPacket.microsecondDuration;
```

`timestamp` 和 `duration` 都以浮点数形式给出。

::: warning
时间戳可能为[负数](#negative-timestamps)。
:::

***

数据包还有一个称为*序列号*的量：

```ts
encodedPacket.sequenceNumber; // => number
```

当[从输入文件读取数据包](./media-sinks#encodedpacketsink)时，这个数字指定了数据包的相对顺序。如果数据包 $A$ 的序列号小于数据包 $B$，那么数据包 $A$ 排在前面（按[解码顺序](./media-sinks#decode-vs-presentation-order)）。如果两个数据包具有相同的序列号，则表示它们代表相同的媒体样本。

序列号本身没有意义，只有在与其他序列号比较时才有意义。如果一个数据包的序列号是 $n$，并不意味着它是轨道的第 $n$ 个数据包。

负的序列号表示数据包的顺序未定义。创建 `EncodedPacket` 时，序列号默认为 -1。

### 克隆数据包

使用 `clone` 方法可以从现有数据包创建新数据包。在此过程中，可以更改其时间戳和持续时间。

```ts
// 创建与原始数据包完全相同的克隆：
packet.clone();

// 创建时间戳设置为 10 秒的克隆：
packet.clone({ timestamp: 10 });
```

### 仅含元数据的数据包

[`EncodedPacketSink`](./media-sinks#encodedpacketsink) 可以创建*仅含元数据*的数据包：

```ts
await sink.getFirstPacket({ metadataOnly: true });
```

仅含元数据的数据包包含完整数据包的所有元数据，但不包含任何实际数据：

```ts
packet.data; // => Uint8Array([])
```

你仍然可以获取数据*原本应有的大小*：

```ts
packet.byteLength; // => number
```

对于任意数据包，你可以这样检查它是否是仅含元数据的：

```ts
packet.isMetadataOnly; // => boolean
```

## `VideoSample`

视频样本(VideoSample)代表单帧视频画面。它可以直接从图像源创建，也可以是解码过程的产物。其 API 设计参考了 [VideoFrame](https://developer.mozilla.org/en-US/docs/Web/API/VideoFrame)。

### 创建视频样本

视频样本有图像源构造函数和原始数据构造函数两种创建方式。

::: info
`VideoSample` 的构造函数与 [`VideoFrame` 的构造函数](https://developer.mozilla.org/en-US/docs/Web/API/VideoFrame/VideoFrame)非常相似，但使用秒级时间戳而非微秒级时间戳。
:::

#### 图像源构造函数

该构造函数从 `CanvasImageSource` 创建 `VideoSample`：

```ts
import { VideoSample } from 'mediabunny';

// 从 canvas 元素创建样本
const sample = new VideoSample(canvas, {
	timestamp: 3, // 单位：秒
	duration: 1/24, // 单位：秒
});

// 从图片元素创建样本，并添加旋转
const sample = new VideoSample(imageElement, {
	timestamp: 5, // 单位：秒
	rotation: 90, // 顺时针角度
});

// 从 VideoFrame 创建样本（时间戳会被复制）
const sample = new VideoSample(videoFrame);
```

#### 原始构造函数

该构造函数通过 `ArrayBuffer` 提供的原始像素数据创建 `VideoSample`：

```ts
import { VideoSample } from 'mediabunny';

// 从 RGBX 格式的像素数据创建样本
const sample = new VideoSample(buffer, {
	format: 'RGBX',
	codedWidth: 1280,
	codedHeight: 720,
	timestamp: 0,
});

// 从 YUV 4:2:0 格式的像素数据创建样本
const sample = new VideoSample(buffer, {
	format: 'I420',
	codedWidth: 1280,
	codedHeight: 720,
	timestamp: 0,
});
```

关于 WebCodecs 支持的像素格式列表，请参阅 [`VideoPixelFormat`](https://w3c.github.io/webcodecs/#enumdef-videopixelformat)。

### 检查视频样本

`VideoSample` 具有以下只读属性：

```ts
// 帧存储的内部像素格式
videoSample.format; // => VideoPixelFormat | null

// 样本的原始尺寸
videoSample.codedWidth; // => number
videoSample.codedHeight; // => number

// 样本的显示尺寸（旋转后）
videoSample.displayWidth; // => number
videoSample.displayHeight; // => number

// 样本的顺时针旋转角度（度）。呈现时应将原始样本旋转该角度。
videoSample.rotation; // => 0 | 90 | 180 | 270

// 时间信息
videoSample.timestamp; // => 呈现时间戳（秒）
videoSample.duration; // => 持续时间（秒）
videoSample.microsecondTimestamp; // => 呈现时间戳（微秒）
videoSample.microsecondDuration; // => 持续时间（微秒）

// 样本的色彩空间
videoSample.colorSpace; // => VideoColorSpace
```

虽然这些属性都是只读的，但你可以使用 `setTimestamp`、`setDuration` 和 `setRotation` 方法来修改视频样本的部分元数据。

::: warning
时间戳可以是[负值](#negative-timestamps)。
:::

### 使用视频样本

视频样本提供了几种访问其帧数据的方式。

您可以将视频样本转换为 WebCodecs 的 [`VideoFrame`](https://developer.mozilla.org/en-US/docs/Web/API/VideoFrame) 以访问额外数据或传递给 [`VideoEncoder`](https://developer.mozilla.org/en-US/docs/Web/API/VideoEncoder)：

```ts
videoSample.toVideoFrame(); // => VideoFrame
```

如果视频样本是使用 `VideoFrame` 构造的，此方法几乎不会产生额外开销。

::: warning
此方法返回的 `VideoFrame` **必须** 与视频样本分开关闭。
:::

***

将视频样本绘制到 `<canvas>` 元素或 `OffscreenCanvas` 上也是常见需求。为此，您可以使用以下方法：

```ts
draw(
	context: CanvasRenderingContext2D | OffscreenCanvasRenderingContext2D,
	dx: number,
	dy: number,
	dWidth?: number, // 默认为 displayWidth
	dHeight?: number, // 默认为 displayHeight
): void;

draw(
	context: CanvasRenderingContext2D | OffscreenCanvasRenderingContext2D,
	sx: number,
	sy: number,
	sWidth: number,
	sHeight: number,
	dx: number,
	dy: number,
	dWidth?: number, // 默认为 sWidth
	dHeight?: number, // 默认为 sHeight
): void;
```

这些方法的行为类似于 [drawImage](https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/drawImage)，会按照给定的位置和尺寸绘制视频帧。此方法会根据帧的 `rotation` 属性自动应用正确的旋转。

`drawWithFit` 方法可用于使用指定的适配算法将视频样本绘制到整个画布上：

```ts
drawWithFit(
	context: CanvasRenderingContext2D | OffscreenCanvasRenderingContext2D,
	options: {
		fit: 'fill' | 'contain' | 'cover';
		rotation?: Rotation; // 覆盖样本的旋转设置
		crop?: CropRectangle;
	},
): void;
```

如果您想直接将原始底层图像绘制到画布上（不考虑旋转元数据），可以使用以下方法：

```ts
videoSample.toCanvasImageSource(); // => VideoFrame | OffscreenCanvas;
```

此方法返回一个有效的 `CanvasImageSource`，可用于 [drawImage](https://developer.mozilla.org/en-US/docs/Web/APICanvasRenderingContext2D/drawImage)。

::: warning
如果此方法返回 `VideoFrame`，您应立即使用该帧。因为任何内部创建的视频帧都会在下一个微任务中自动关闭。
:::

***

有时您可能需要直接访问底层像素数据。为此，`VideoSample` 允许您将这些数据复制到 `ArrayBuffer` 中。

使用 `allocationSize` 确定所需的字节数：

```ts
const bytesNeeded = videoSample.allocationSize(); // => number
```

然后使用 `copyTo` 将像素数据复制到目标缓冲区：

```ts
const bytes = new Uint8Array(bytesNeeded);
videoSample.copyTo(bytes);
```

::: info
数据始终采用 `format` 字段中指定的像素格式。

要将数据转换为其他像素格式，或仅提取帧的某一部分，请改用 `VideoFrame` 的 [`allocationSize`](https://developer.mozilla.org/en-US/docs/Web/API/VideoFrame/allocationSize) 和 [`copyTo`](https://developer.mozilla.org/en-US/docs/Web/API/VideoFrame/copyTo) 方法。通过运行 `videoSample.toVideoFrame()` 获取 `VideoFrame`。
:::

***

您还可以克隆 `VideoSample`：

```ts
const clonedSample = videoSample.clone(); // => VideoSample
```

克隆的样本**必须**与原始样本分开关闭。

### 关闭视频样本

使用完 `VideoSample` 后，必须手动关闭它以释放内部占用的资源。通过调用 `close` 方法实现：

```ts
videoSample.close();
```

请尽量在不再需要 `VideoSample` 时立即关闭它。未关闭的视频样本可能导致显存占用过高和解码器停滞。

`VideoSample` 关闭后，其数据将不可用，且大多数方法会抛出错误。

## `AudioSample`

音频样本代表一段音频数据。它可以直接从原始音频数据创建，也可以是解码过程的结果。其 API 设计参考了 [AudioData](https://developer.mozilla.org/en-US/docs/Web/API/AudioData)。

### 创建音频样本

音频样本可以通过 `AudioData` 实例、初始化对象或 `AudioBuffer` 来构建：

```ts
import { AudioSample } from 'mediabunny';

// 从 AudioData 创建：
const sample = new AudioSample(audioData);

// 从原始数据创建：
const sample = new AudioSample({
    data: new Float32Array([...]),
    format: 'f32-planar', // 音频样本格式
    numberOfChannels: 2,
    sampleRate: 44100, // 单位 Hz
    timestamp: 0, // 单位秒
});

// 从 AudioBuffer 创建：
const timestamp = 0; // 单位秒
const samples = AudioSample.fromAudioBuffer(audioBuffer, timestamp);
// => 如果 AudioBuffer 很长，会返回多个 AudioSample
```

支持以下音频样本格式：

* `'u8'`: 8位无符号整数（交错格式）
* `'u8-planar'`: 8位无符号整数（平面格式）
* `'s16'`: 16位有符号整数（交错格式）
* `'s16-planar'`: 16位有符号整数（平面格式）
* `'s32'`: 32位有符号整数（交错格式）
* `'s32-planar'`: 32位有符号整数（平面格式）
* `'f32'`: 32位浮点数（交错格式）
* `'f32-planar'`: 32位浮点数（平面格式）

平面格式将每个声道的数据连续存储，而交错格式将各声道的数据交错存储：

![平面格式 vs. 交错格式](../assets/planar_interleaved.svg)

### 音频样本检查

`AudioSample` 具有多个只读属性：

```ts
type AudioSampleFormat = 'u8' | 'u8-planar' | 's16' | 's16-planar' | 's32' | 's32-planar' | 'f32' | 'f32-planar';

audioSample.format; // => AudioSampleFormat
audioSample.sampleRate; // => 采样率（Hz）
audioSample.numberOfFrames; // => 每通道的帧数
audioSample.numberOfChannels; // => 通道数
audioSample.timestamp; // => 呈现时间戳（秒）
audioSample.duration; // => 持续时间（秒）（= numberOfFrames / sampleRate）

// 还存在时间信息的微秒整数版本：
audioSample.microsecondTimestamp;
audioSample.microsecondDuration;
```

虽然所有这些属性都是只读的，但您可以使用 `setTimestamp` 方法来修改音频样本的时间戳。

::: warning
时间戳可以是[负数](#negative-timestamps)。
:::

### 使用音频样本

音频样本提供了几种访问其音频数据的方式。

您可以将音频样本转换为 WebCodecs 的 [`AudioData`](https://developer.mozilla.org/en-US/docs/Web/API/AudioData)，以便传递给 [`AudioEncoder`](https://developer.mozilla.org/en-US/docs/Web/API/AudioEncoder)：

```ts
audioSample.toAudioData(); // => AudioData
```

如果音频样本是使用 `AudioData` 构造的，只要没有使用 `setTimestamp` 修改其时间戳，此方法几乎是零成本的。

::: warning
此方法返回的 `AudioData` **必须** 与音频样本分开关闭。
:::

您还可以轻松地将音频样本转换为 [`AudioBuffer`](https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer)，以便与 Web Audio API 一起使用：

```ts
audioSample.toAudioBuffer(); // => AudioBuffer
```

***

您也可以直接将原始音频数据从 `AudioSample` 复制到 `ArrayBuffer`。为此，可以使用 `allocationSize` 和 `copyTo` 方法。

复制过程由以下配置对象控制：

```ts
type AudioSampleCopyToOptions = {
	planeIndex: number;
	format?: AudioSampleFormat;
	frameOffset?: number;
	frameCount?: number;
};
```

* `planeIndex` *(必填)*\
  标识要复制的平面的索引。如果使用非平面（交错）输出格式，则必须为 0。
* `format`\
  目标数据的输出格式。默认为 `AudioSample` 的格式。
* `frameOffset`\
  源平面数据中的偏移量，指示从哪一帧开始复制。默认为 0。
* `frameCount`\
  要复制的帧数。如果未提供，复制将包括从 `frameOffset` 开始的所有帧。

由于 `planeIndex` 的含义取决于提取时使用的格式，最好始终明确指定格式。

例如，这里我们将整个音频数据提取为 `f32`：

```ts
const options = { planeIndex: 0, format: 'f32' };
const bytesNeeded = audioSample.allocationSize(options);
const data = new Float32Array(bytesNeeded / 4);
audioSample.copyTo(data, options);
```

这里，我们遍历 `s16` 格式的每个平面：

```ts
// 第一个平面的大小就是所有平面的大小
const bytesNeeded = audioSample.allocationSize({
	planeIndex: 0,
	format: 's16-planar',
});
const data = new Int16Array(bytesNeeded / 2);

for (let i = 0; i < audioSample.numberOfChannels; i++) {
	audioSample.copyTo(data, { planeIndex: i, format: 's16-planar' });
	// 对数据进行处理
}
```

::: info
`allocationSize` 和 `copyTo` 的行为与 WebCodecs API 完全一致。然而，WebCodecs API 规范仅要求支持转换为 `f32-planar`，而 Mediabunny 的实现支持转换为所有格式。因此，Mediabunny 的方法更加强大。
:::

### 关闭音频样本

使用完 `AudioSample` 后，必须手动关闭它以释放内部持有的资源。通过调用 `close` 方法实现：

```ts
audioSample.close();
```

请尽量在不再需要 `AudioSample` 时立即关闭它。未关闭的音频样本可能导致高内存占用和解码器停滞。

`AudioSample` 关闭后，其数据将不可用，且大部分方法会抛出错误。

---

---
url: /examples.md
---


---

---
url: /guide/introduction.md
---
# 简介

Mediabunny 是一个 JavaScript 库，用于直接在浏览器中读取、写入和转换媒体文件（如 MP4 或 WebM）。它旨在成为 Web 上进行高性能媒体操作的完整工具包。该库完全使用纯 TypeScript 从零开始编写，零依赖，并且具有极好的 tree-shaking 特性，这意味着您只包含您使用的部分。您可以将其视为类似于 [FFmpeg](https://ffmpeg.org/) 的工具，但专为 Web 需求而构建。

## 功能特性

以下是该库支持的一系列功能：

* 从媒体文件中读取元数据
* 从媒体文件中提取媒体数据
* 创建新的媒体文件
* 转换媒体文件
* 硬件加速解码和编码（通过 WebCodecs API）
* 支持多个视频、音频和字幕轨道
* 支持多种容器格式的读写（.mp4、.mov、.webm、.mkv、.mp3、.wav、.ogg、.aac），包括 MP4 快速启动、分段 MP4 或可流式传输的 Matroska 等变体
* 支持 25 种不同的编解码器
* 延迟、优化、按需的文件读取
* 输入和输出流式传输，支持任意文件大小
* 文件位置无关性（内存、磁盘、网络等）
* 压缩、调整大小、旋转、裁剪、重采样、修剪等实用工具
* 转封装和转码
* 微秒级的读写精度
* 高效的时间定位（Seeking）
* 流水线设计，实现高效的硬件使用和自动背压处理
* 支持自定义编码器和解码器以进行 polyfill
* 针对不同用例提供低层级和高层级的抽象
* 所有操作均具备高性能
* 支持 Node.js

……可能还有更多功能。

## 使用场景

Mediabunny 是一个通用工具包，可用于无限多种场景。以下是一些应用思路：

* 文件转换与压缩
* 显示文件元数据（时长、尺寸等）
* 提取缩略图
* 在浏览器中创建视频
* 构建视频编辑器
* 实时录制与流媒体传输
* 通过 Web Audio API 实现大型文件的高精度采样播放

欢迎查看 [示例](/examples) 页面，了解这些想法的具体实现演示！

## 快速开始

要开始使用 Mediabunny，可以从以下入口点着手：

* 查看 [快速入门](./quick-start)，获取实用代码片段集合
* 如需进行读取操作，请从 [读取媒体文件](./reading-media-files) 开始
* 如需进行写入操作，请从 [写入媒体文件](./writing-media-files) 开始
* 若关注文件转换，请从 [转换媒体文件](./converting-media-files) 开始
* 深入 [数据包与采样](./packets-and-samples) 章节，深入了解该库的核心概念

## 开发动机

Mediabunny 是我之前开发的 [mp4-muxer](https://github.com/Vanilagy/mp4-muxer) 和 [webm-muxer](https://github.com/Vanilagy/webm-muxer) 库的演进版本。这两个库都是随着 WebCodecs API 的出现而创建的。虽然它们功能完备，但我发现了以下痛点：

* 两个库之间存在大量重复代码，API 却非常相似
* 未能帮助开发者克服使用 WebCodecs API 及相关浏览器 API 的困难
* 用户持续追问 "mp4-demuxer 什么时候推出？"

本库通过整合这两个库，解决了上述所有问题，并扩展了功能范围。现在：

* 更改输出文件格式只需单行代码修改，其余 API 保持完全一致
* 提供了基于 WebCodecs API 和浏览器 API 的多层抽象
* 正式推出了 mp4-demuxer 功能

得益于摇树优化（tree shaking），如果您仅需 MP4 或 WebM 封装器，本库的打包体积仍然会非常小。

### 迁移指南

如果您正在从 mp4-muxer 或 webm-muxer 迁移，建议迁移至 Mediabunny。请参考以下迁移指南：

* [指南：从 mp4-muxer 迁移至 Mediabunny](https://github.com/Vanilagy/mp4-muxer/blob/main/MIGRATION-GUIDE.md)
* [指南：从 webm-muxer 迁移至 Mediabunny](https://github.com/Vanilagy/webm-muxer/blob/main/MIGRATION-GUIDE.md)

## 技术概述

Mediabunny 的核心是一组多路复用器（multiplexer）与多路解复用器（demultiplexer），每种容器格式都对应一组。多路解复用器从*源*流式读取数据，而多路复用器则将数据流式写入*目标*。每个多路解复用器都能提取文件元数据和压缩后的媒体数据，而多路复用器则负责将元数据和编码后的媒体数据写入新文件。

Mediabunny 还在 WebCodecs API 的基础上提供了多个封装层以简化使用：在读取方面，它会自动创建带有正确编解码器配置的解码器，并以流水线方式高效解码媒体数据；在写入方面，它会推断所需的编解码器配置并设置编码器，用于对原始媒体数据进行编码，同时遵循编码器施加的反压机制。从媒体文件中提取正确的解码器配置可能较为复杂，有时甚至需要深入分析已编码媒体数据包的比特流。

转换抽象层建立在 Mediabunny 的读取和写入原语之上，并以高度流水线化的方式将二者结合，确保读取和写入操作同步进行。该层还包含大量条件逻辑，用于探测输出轨道的兼容性、解码支持情况，并寻找可编码的编解码器配置。它利用 Canvas API 进行视频处理操作，并使用自定义实现进行音频重采样及上下混音。

---

---
url: /guide/reading-media-files.md
---
# 读取媒体文件

Mediabunny 让您能够以高效可控的方式读取媒体文件。您可以使用它来提取元数据（如时长或分辨率），以及从视频和音频轨道中以帧精确的时序读取实际媒体数据。支持多种常用的[输入文件格式](./input-formats)。通过[输入源](#input-sources)，可以从多种来源读取数据，包括直接从内存、用户磁盘甚至通过网络读取。

文件总是以"惰性"方式部分读取，这意味着只会读取提取所需信息必要的字节，从而保持高性能和低内存占用。因此，大多数读取数据的方法都是异步的并返回 Promise。

::: info
并非所有数据的提取方式都相同。以 `compute` 而非 `get` 开头的方法表明库可能需要执行更多工作来获取请求的数据。
:::

## 创建新的输入

在 Mediabunny 中读取媒体文件的核心是围绕 `Input` 类展开的，所有读取操作都从这里开始。一个 `Input` 实例代表一个我们要读取的媒体文件。

首先创建一个新的 `Input` 实例。这里我们使用 [File](https://developer.mozilla.org/en-US/docs/Web/API/File) 实例来创建，意味着我们将直接从用户磁盘读取数据：

```ts
import { Input, ALL_FORMATS, BlobSource } from 'mediabunny';

const input = new Input({
	formats: ALL_FORMATS,
	source: new BlobSource(file),
});
```

`source` 指定了 `Input` 读取数据的来源。完整可用的输入源列表请参阅 [输入源](#input-sources)。

`formats` 指定了 `Input` 应该支持的格式列表。这个字段主要用于 tree shaking 优化：使用 `ALL_FORMATS` 意味着我们可以加载 [Mediabunny 支持的所有格式](./supported-formats-and-codecs#container-formats) 的文件，但需要包含所有这些格式的解析器。如果我们确定只会读取 MP3 或 WAVE 文件，那么像下面这样设置可以大幅减少整体打包体积：

```ts
import { Input, MP3, WAVE } from 'mediabunny';

const input = new Input({
	formats: [MP3, WAVE],
	// ....
});
```

如果无法识别文件格式，读取操作会抛出错误。完整可用的输入格式列表请参阅 [输入格式](./input-formats)。

::: info
仅仅创建 `Input` 实例不会执行任何读取操作，实际上几乎没有开销。只有在请求数据时才会真正读取文件。
:::

## 读取文件元数据

创建 `Input` 实例后，您现在可以开始读取文件级别的元数据。

您可以像这样查询文件的具体格式：

```ts
await input.getFormat(); // => Mp4InputFormat
```

您可以直接获取文件的完整 MIME 类型，包括轨道编解码器信息：

```ts
await input.getMimeType(); // => 'video/mp4; codecs="avc1.42c032, mp4a.40.2"'
```

使用 `computeDuration` 可以获取媒体文件的总时长（以秒为单位）：

```ts
await input.computeDuration(); // => 1905.4615
```

更具体地说，时长定义为所有轨道中最大的结束时间戳。

Mediabunny 还允许您从媒体文件中读取描述性元数据标签，如标题、艺术家或封面图：

```ts
await input.getMetadataTags(); // => MetadataTags
```

更多信息，请参阅 [`MetadataTags`](../api/MetadataTags)。

## 读取轨道元数据

您可以像这样提取文件中所有媒体轨道的列表：

```ts
await input.getTracks(); // => InputTrack[]
```

还有一些用于检索轨道的实用方法可能很有用：

```ts
await input.getVideoTracks(); // => InputVideoTrack[]
await input.getAudioTracks(); // => InputAudioTrack[]

await input.getPrimaryVideoTrack(); // => InputVideoTrack | null
await input.getPrimaryAudioTrack(); // => InputAudioTrack | null
```

::: info
目前不支持读取字幕轨道。
:::

### 常见轨道元数据

获取 `InputTrack` 后，您就可以开始从中提取元数据：

```ts
// 获取该轨道在输入文件中的唯一ID：
track.id; // => number

// 检查轨道类型：
track.type; // => 'video' | 'audio' | 'subtitle';

// 或者使用这些类型断言方法：
track.isVideoTrack(); // => boolean
track.isAudioTrack(); // => boolean

// 以ISO 639-2/T语言代码获取轨道语言。
// 如果语言未知则返回'und'（未确定）
track.languageCode; // => string

// 该轨道的用户定义名称
track.name; // => string
```

#### 编解码器信息

您可以查询与轨道编解码器相关的元数据：

```ts
track.codec; // => MediaCodec | null
```

当轨道的编解码器无法识别或Mediabunny不支持时，此字段为`null`。完整支持的编解码器列表请参阅[编解码器](./supported-formats-and-codecs#codecs)。当Mediabunny无法识别格式时，您仍可以使用`internalCodecId`字段来确定轨道的编解码器，但其格式取决于所使用的容器格式，且Mediabunny不会对其进行统一处理。

您还可以从轨道中提取完整的编解码器参数字符串，该字符串遵循[WebCodecs编解码器注册表](https://www.w3.org/TR/webcodecs-codec-registry/)规范：

```ts
await track.getCodecParameterString(); // => 'avc1.42001f'
```

即使知道编解码器，也不意味着用户的浏览器能够解码它。要检查可解码性，请使用`canDecode`：

```ts
await track.canDecode(); // => boolean
```

::: info
此检查还会考虑[自定义解码器](./supported-formats-and-codecs#custom-decoders)。
:::

#### 跟踪时间信息

您可以通过以下方式计算轨道特定的持续时间（以秒为单位）：

```ts
await track.computeDuration(); // => 1902.4615
```

与 `Input` 的持续时间类似，这与最后一个样本的结束时间戳相同。如果 `Input` 包含长度不同的多个轨道，则轨道的持续时间可能比 `Input` 的总持续时间短。

您还可以获取轨道的*起始时间戳*（以秒为单位）：

```ts
await track.getFirstTimestamp(); // => 0.041666666666666664
```

这与*持续时间*相反：它是第一个样本的起始时间戳。

::: warning
轨道的起始时间戳**不一定**为 0。它通常接近零，但也可能是略微正值，甚至是略微负值。

*正的起始时间戳*表示第一个样本在整体组合开始*之后*呈现。如果这是视频轨道，您可以选择显示占位图像（如黑屏），或者在第二帧开始前将第一帧作为冻结帧显示。

*负的起始时间戳*表示轨道在组合开始*之前*已经开始；这实际上意味着媒体数据的某些起始部分被"截断"。建议不要显示时间戳为负的样本。
:::

另一个与轨道时间信息相关的指标是它的*时间分辨率*，以赫兹为单位：

```ts
track.timeResolution; // => 24
```

直观地说，这是轨道可能的最高"帧率"（假设没有两个样本具有相同的时间戳）。从数学上讲，如果 $x$ 等于轨道的时间分辨率，则该轨道的所有时间戳和持续时间可以表示为：

$$ \frac{k}{x},\quad k \in \mathbb{Z} $$

::: info
此字段仅给出轨道帧率的上限。要根据样本获取轨道的实际帧率，请计算其[数据包统计信息](#packet-statistics)。
:::

#### 数据包统计

您可以查询轨道编码数据包的聚合统计信息：

```ts
await track.computePacketStats(); // => PacketStats

type PacketStats = {
	// 数据包总数
	packetCount: number;
	// 每秒平均数据包数量
	// 对于视频轨道，这将等于平均帧率(FPS)
	averagePacketRate: number;
	// 每秒平均比特数
	averageBitrate: number;
};
```

例如，在1080p版本的《Big Buck Bunny》视频轨道上运行此方法会返回：

```ts
{
	packetCount: 14315,
	averagePacketRate: 24,
	averageBitrate: 9282573.233670976,
}
```

这意味着该视频轨道总共有14315帧，帧率精确为24Hz，平均比特率约为9.28Mbps。

**注意：** 这些统计信息并非直接从文件元数据中读取，而是需要计算得出，因此该方法可能需要执行多次读取（具体取决于文件大小），可能需要数百毫秒才能完成。为了加速计算，您可以通过传递参数来仅计算部分数据包的聚合统计：

```ts
await track.computePacketStats(50);
```

这将仅检查前约50个数据包后返回结果。这对于快速估算帧率和比特率非常有用，无需扫描整个文件。对于恒定帧率的视频，这种方法也总能返回正确的帧率。

### 视频轨道元数据

除了[通用轨道元数据](#common-track-metadata)外，视频轨道还包含可查询的额外元数据：

```ts
// 获取轨道编码样本的原始像素尺寸（旋转前）：
videoTrack.codedWidth; // => number
videoTrack.codedHeight; // => number

// 获取轨道样本显示时的像素尺寸（旋转后）：
videoTrack.displayWidth; // => number
videoTrack.displayHeight; // => number

// 获取轨道帧应顺时针旋转的角度（度数）：
videoTrack.rotation; // => 0 | 90 | 180 | 270
```

要计算视频轨道的平均帧率(FPS)，可使用[`computePacketStats`](#packet-statistics)：

```ts
const stats = await videoTrack.computePacketStats(100);
const frameRate = stats.averagePacketRate; // 近似值，但通常精确
```

可以获取轨道的解码器配置，这是一个来自WebCodecs API的`VideoDecoderConfig`，用于`VideoDecoder`：

```ts
await videoTrack.getDecoderConfig(); // => VideoDecoderConfig | null
```

如果轨道的编解码器未知，此方法可能返回`null`。

例如，以下是Big Buck Bunny的1080p版本的解码器配置：

```ts
{
	codec: 'avc1.4d4029',
	codedWidth: 1920,
	codedHeight: 1080,
	description: new Uint8Array([
		// AVCDecoderConfigurationRecord的字节数据
		1, 77, 64, 41, 255, 225, 0, 22, 39, 77, 64, 41, 169, 24, 15, 0,
		68, 252, 184, 3, 80, 16, 10, 27, 108, 43, 94, 247, 192, 64, 1, 0,
		4, 40, 222, 9, 200,
	]),
}
```

可以直接获取视频色彩空间的信息：

```ts
await videoTrack.getColorSpace(); // => VideoColorSpaceInit
```

如果色彩空间信息未知，返回的对象将包含`undefined`值。

还可以直接检查视频是否具有\_高动态范围\_(HDR)：

```ts
await videoTrack.hasHighDynamicRange(); // => boolean
```

此方法会与可用的色彩空间元数据进行比较。如果返回`true`，则表示视频是HDR；如果返回`false`，则视频可能是也可能不是HDR。

### 音频轨道元数据

除了[通用轨道元数据](#common-track-metadata)外，音频轨道还包含以下可查询的额外元数据：

```ts
// 获取音频通道数：
audioTrack.numberOfChannels; // => number

// 获取音频采样率（单位赫兹）：
audioTrack.sampleRate; // => number
```

你可以获取轨道的解码器配置，这是一个来自 WebCodecs API 的 `AudioDecoderConfig`，可用于 `AudioDecoder`：

```ts
await audioTrack.getDecoderConfig(); // => AudioDecoderConfig | null
```

如果轨道的编解码器未知，该方法可能返回 `null`。

例如，以下是一个 AAC 音频轨道的解码器配置：

```ts
{
	codec: 'mp4a.40.2',
	numberOfChannels: 2,
	sampleRate: 44100,
	description: new Uint8Array([
		// AudioSpecificConfig 的字节数据
		17, 144,
	]),
}
```

## 读取媒体数据

Mediabunny 采用了\*媒体接收器（media sinks）\*的概念，这是从 `InputTrack` 读取媒体数据的方式。不同的媒体接收器在 API 和抽象级别上有所差异，你可以选择最适合使用场景的接收器。

完整接收器列表请参阅[媒体接收器](./media-sinks)。

### 示例

遍历轨道所有原始编码数据包：

```ts
import { EncodedPacketSink } from 'mediabunny';

const videoTrack = await input.getPrimaryVideoTrack();
const sink = new EncodedPacketSink(videoTrack);

for await (const packet of sink.packets()) {
	console.log(packet.timestamp);
}
```

这里我们遍历视频轨道的所有样本（帧）：

```ts
import { VideoSampleSink } from 'mediabunny';

const videoTrack = await input.getPrimaryVideoTrack();
const sink = new VideoSampleSink(videoTrack);

for await (const sample of sink.samples()) {
	// 例如，我们可以将样本绘制到画布上：
	sample.draw(ctx, 0, 0);
}
```

我们还可以更具体地使用这个接收器：

```ts
// 遍历时间戳在300秒到305秒之间的所有帧
for await (const sample of sink.samples(300, 305)) {
	// ...
}

// 获取时间戳42秒处显示的帧
await sink.getSample(42);
```

我们可能想从视频轨道提取缩略图：

```ts
import { CanvasSink } from 'mediabunny';

const videoTrack = await input.getPrimaryVideoTrack();
const sink = new CanvasSink(videoTrack, {
	width: 320,
	height: 180,
});

const startTimestamp = await videoTrack.getFirstTimestamp();
const endTimestamp = await videoTrack.computeDuration();

// 生成五个等间距的缩略图：
const thumbnailTimestamps = [0, 0.2, 0.4, 0.6, 0.8].map(
	(t) => startTimestamp + t * (endTimestamp - startTimestamp),
);

for await (const result of sink.canvasesAtTimestamps(thumbnailTimestamps)) {
	// 在缩略图上添加MrBeast的脸
}
```

我们可以遍历音频轨道的一个片段并使用Web Audio API播放：

```ts
import { AudioBufferSink } from 'mediabunny';

const audioTrack = await input.getPrimaryAudioTrack();
const sink = new AudioBufferSink(audioTrack);

for await (const { buffer, timestamp } of sink.buffers(5, 10)) {
	const node = audioContext.createBufferSource();
	node.buffer = buffer;
	node.connect(audioContext.destination);
	node.start(timestamp);
}
```

或者我们可以手动控制解码过程：

```ts
import { EncodedPacketSink } from 'mediabunny';

const videoTrack = await input.getPrimaryVideoTrack();
const sink = new EncodedPacketSink(videoTrack);

const decoder = new VideoDecoder({
	output: console.log,
	error: console.error,
});
decoder.configure(await videoTrack.getDecoderConfig());

// 处理从37秒到50秒的所有数据包：
let currentPacket = await sink.getKeyPacket(37);
while (currentPacket && currentPacket.timestamp < 50) {
	decoder.decode(currentPacket.toEncodedVideoChunk());
	currentPacket = await sink.getNextPacket(currentPacket);
}

await decoder.flush();
```

如你所见，媒体接收器非常灵活，可以高效、稀疏地读取输入文件中的媒体数据。

## 输入源

*input source*（输入源）决定了 `Input` 从何处读取数据。

所有输入源都有一个 `onread` 回调属性，你可以通过它来检查文件被读取的区域：

```ts
source.onread = (start, end) => {
	console.log(`正在读取字节范围 [${start}, ${end})`);
};
```

***

该库提供了几种数据源类型：

### `BufferSource`

此数据源使用内存中的 `ArrayBuffer` 作为底层数据源。

```ts
import { BufferSource } from 'mediabunny';

// 可以直接从 ArrayBuffer 构造 BufferSource：
const source = new BufferSource(arrayBuffer);

// 也可以从 Uint8Array 构造：
const source = new BufferSource(uint8Array);
```

这是最快的数据源，但需要将整个输入文件保存在内存中。

### `BlobSource`

此数据源基于底层的 [`Blob`](https://developer.mozilla.org/en-US/docs/Web/API/Blob) 对象。由于 [`File`](https://developer.mozilla.org/en-US/docs/Web/API/File) 继承自 `Blob`，此数据源非常适合直接从磁盘读取数据（在浏览器中）。

```ts
import { BlobSource } from 'mediabunny';

fileInput.addEventListener('change', (event) => {
	const file = event.target.files[0];
	const source = new BlobSource(file);
});
```

`BlobSource` 接受第二个参数作为额外选项：

```ts
type BlobSourceOptions = {
	// 缓存允许在内存中保留的最大字节数
	// 默认为 8 MiB
	maxCacheSize?: number;
};
```

### `UrlSource`

此数据源从远程 URL 获取数据，适用于通过网络读取文件。

```ts
import { UrlSource } from 'mediabunny';

const source = new UrlSource('https://example.com/bigbuckbunny.mp4');
```

`UrlSource` 会根据观察到的访问模式智能地预取数据，以减少请求次数和延迟。

::: warning
如果在浏览器中使用此数据源且 URL 位于不同源，请确保正确配置了 [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS)。
:::

`UrlSource` 接受几个选项作为第二个参数：

```ts
type UrlSourceOptions = {
	requestInit?: RequestInit;
	getRetryDelay?: (previousAttempts: number) => number | null;

	// 缓存允许在内存中保留的最大字节数
	// 默认为 8 MiB
	maxCacheSize?: number;
};
```

可以像 Fetch API 一样使用 `requestInit` 进一步自定义请求：

```ts
const source = new UrlSource('https://example.com/bigbuckbunny.mp4', {
	requestInit: {
		headers: {
			'X-Custom-Header': 'my-value',
		},
	},
});
```

`getRetryDelay` 可用于控制请求失败时的重试逻辑。当请求失败时，`getRetryDelay` 应返回重试前的等待时间（秒）。返回 `null` 会阻止进一步重试。

```ts
// 使用指数退避重试逻辑的 UrlSource：
const source = new UrlSource('https://example.com/bigbuckbunny.mp4', {
	getRetryDelay: (previousAttempts) => Math.min(2 ** previousAttempts, 16),
});
```

不设置 `getRetryDelay` 将默认使用无限的、有上限的指数退避模式。

### `FilePathSource`

此输入源可用于直接从给定文件路径的文件加载数据。需要 Node、Bun 或 Deno 等服务器端环境。

```ts
import { FilePathSource } from 'mediabunny';

const source = new FilePathSource('/home/david/Downloads/bigbuckbunny.mp4');
```

`FilePathSource` 接受第二个参数作为额外选项：

```ts
type FilePathSourceOptions = {
	// 缓存允许在内存中保留的最大字节数
	// 默认为 8 MiB
	maxCacheSize?: number;
};
```

### `StreamSource`

这是一个通用输入源，可用于从任何地方读取数据。

例如，这里我们使用 Node.js 文件系统从磁盘读取文件（尽管你应该使用 [`FilePathSource`](#filepathsource)）：

```ts
import { StreamSource } from 'mediabunny';
import { open } from 'node:fs/promises';

const fileHandle = await open('bigbuckbunny.mp4', 'r');

const source = new StreamSource({
	read: async (start, end) => {
		const buffer = Buffer.alloc(end - start);
		await fileHandle.read(buffer, 0,end - start, start);
		return buffer;
	},
	getSize: async () => {
		const { size } = await fileHandle.stat();
		return size;
	},
});
```

`StreamSource` 的选项有以下类型：

```ts
type StreamSourceOptions = {
	getSize: () => MaybePromise<number>;
	read: (start: number, end: number) => MaybePromise<Uint8Array | ReadableStream<Uint8Array>>;
	maxCacheSize?: number;
	prefetchProfile?: 'none' | 'fileSystem' | 'network';
};

type MaybePromise<T> = T | Promise<T>;
```

* `getSize`\
  当请求整个文件的大小时调用。必须返回或解析为字节大小。此函数保证在 `read` 之前调用。
* `read`\
  当请求数据时调用。必须返回或解析为指定字节范围的字节，或生成这些字节的流。
* `maxCacheSize`\
  缓存允许在内存中保留的最大字节数。默认为 8 MiB。
* `prefetchProfile`\
  指定读取器应与此源一起使用的预取配置文件。预取配置文件指定预加载请求范围之外的字节的模式，以减少未来读取的延迟。
  * `'none'`（默认）：不预取；仅请求当前需要的数据。
  * `'fileSystem'`：文件系统优化的预取：双向预取少量数据，与页面边界对齐。
  * `'network'`：网络优化的预取，或更一般地说，针对任何高延迟环境优化的预取：尝试最小化读取调用次数，并在检测到顺序访问模式时积极预取数据。

### `ReadableStreamSource`

这是一个由 `Uint8Array` 的 `ReadableStream` 支持的数据源，表示未知长度的仅追加字节流。此数据源适用于增量流式传输仍在构建中且大小未知的输入文件。你也可以用它来流式传输现有文件，但建议使用其他数据源（如 [`BlobSource`](#blobsource) 或 [`FilePathSource`](#filepathsource)），因为它们提供随机访问。

```ts
import { ReadableStreamSource } from 'mediabunny';

const { writable, readable } = new TransformStream<Uint8Array, Uint8Array>();
const source = new ReadableStreamSource(readable);

// 追加数据块
const writer = writable.getWriter();
writer.write(chunk1);
writer.write(chunk2);
writer.close();
```

此数据源是*无大小的*，意味着调用 `.getSize()` 会抛出错误，且由于缺乏随机文件访问，读取器的功能更加受限。你应该仅将此数据源用于顺序访问模式，例如从头到尾读取所有数据包或进行转换。除非增加最大缓存大小，否则此数据源不适合随机访问模式。

```ts
type ReadableStreamSourceOptions = {
	// 缓存允许在内存中保留的最大字节数
	// 默认为 16 MiB
	maxCacheSize?: number;
};
```

#### 与 [`MediaRecorder`](https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder) 一起使用

你可以将 `MediaRecorder` 与 `ReadableStreamSource` 结合使用，在录制过程中将录制数据流式传输到 Mediabunny。以下是一个示例，我们将 `MediaRecorder` 的输出通过 Mediabunny 的 Conversion API 转换为 WAVE 文件：

```ts
import {
	Input,
	Output,
	Conversion,
	ReadableStreamSource,
	ALL_FORMATS,
	WavOutputFormat,
	BufferTarget,
} from 'mediabunny';

// 设置 TransformStream 将 MediaRecorder 的 Blob 转换为 Uint8Array
const { writable, readable } = new TransformStream<Blob, Uint8Array>({
	async transform(chunk, controller) {
		const arrayBuffer = await chunk.arrayBuffer();
		controller.enqueue(new Uint8Array(arrayBuffer));
	},
});

const input = new Input({
	source: new ReadableStreamSource(readable),
	formats: ALL_FORMATS,
});
const output = new Output({
	format: new WavOutputFormat(),
	target: new BufferTarget(),
});

const conversionPromise = Conversion.init({ input, output })
	.then(conversion => conversion.execute());

const micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
const recorder = new MediaRecorder(micStream);
const writer = writable.getWriter();

recorder.ondataavailable = e => writer.write(e.data);
recorder.onstop = async () => {
	await writer.close();
	await conversionPromise;

	// 获取最终的 .wav 文件
	const wavFile = output.target.buffer!; // => ArrayBuffer
};

recorder.start(1000);
setTimeout(() => recorder.stop(), 10_000); // 10秒后停止录制
```

***

---

---
url: /guide/input-formats.md
---
# 输入格式

Mediabunny 支持多种常见容器格式用于读取输入文件。这些*输入格式*有两种使用方式：

* 创建 `Input` 时，用于指定支持的容器格式列表。详见[创建新输入](./reading-media-files#creating-a-new-input)。
* 对于已有的 `Input`，其 `getFormat` 方法会返回文件的*实际*格式，类型为 `InputFormat`。

## 输入格式属性

获取格式的完整名称如下：

```ts
inputFormat.name; // => 'MP4'
```

也可以获取格式的基础 MIME 类型：

```ts
inputFormat.mimeType; // => 'video/mp4'
```

如果需要包含轨道编解码器的完整 MIME 类型，请改用 `Input` 的 [`getMimeType`](./reading-media-files#reading-file-metadata) 方法。

## 输入格式单例对象

由于输入格式不需要任何额外配置，每个输入格式都直接作为导出的单例实例提供：

```ts
import {
	MP4, // MP4 输入格式单例
	QTFF, // QuickTime 文件格式输入格式单例
	MATROSKA, // Matroska 输入格式单例
	WEBM, // WebM 输入格式单例
	MP3, // MP3 输入格式单例
	WAVE, // WAVE 输入格式单例
	OGG, // Ogg 输入格式单例
} from 'mediabunny';
```

在创建输入时可以使用这些单例对象：

```ts
import { Input, MP3, WAVE, OGG } from 'mediabunny';

const input = new Input({
	formats: [MP3, WAVE, OGG],
	// ...
});
```

也可以使用它们来检查 `Input` 的实际格式：

```ts
import { MP3 } from 'mediabunny';

const isMp3 = (await input.getFormat()) === MP3;
```

Mediabunny 导出了一个特殊的 `ALL_FORMATS` 常量，它包含所有输入格式单例。如果您需要支持尽可能多的格式，可以使用这个常量：

```ts
import { Input, ALL_FORMATS } from 'mediabunny';

const input = new Input({
	formats: ALL_FORMATS,
	// ...
});
```

::: info
使用 `ALL_FORMATS` 意味着所有格式的[解复用器](https://en.wikipedia.org/wiki/Demultiplexer_\(media_file\))都必须包含在打包文件中，这会显著增加打包体积。仅在需要支持所有格式时使用。
:::

## 输入格式类的层次结构

除了单例外，输入格式类采用层次化结构：

* `InputFormat` (抽象类)
  * `IsobmffInputFormat` (抽象类)
    * `Mp4InputFormat`
    * `QuickTimeInputFormat`
  * `MatroskaInputFormat`
    * `WebMInputFormat`
  * `Mp3InputFormat`
  * `WaveInputFormat`
  * `OggInputFormat`

这意味着你也可以使用 `instanceof` 而非 `===` 比较来执行输入格式检查。例如：

```ts
import { Mp3InputFormat } from 'mediabunny';

// 检查文件是否为 MP3：
(await input.getFormat()) instanceof Mp3InputFormat;

// 检查文件是否为 Matroska (MKV + WebM)：
(await input.getFormat()) instanceof MatroskaInputFormat;

// 检查文件是否为 MP4 或 QuickTime：
(await input.getFormat()) instanceof IsobmffInputFormat;
```

::: info
严谨地说 🤓☝️，QuickTime 文件格式(QTFF)在技术上并不是 ISO 基础媒体文件格式(ISOBMFF)的实例——实际上 ISOBMFF 是最初受 QTFF 启发而制定的标准。但由于两者极其相似且使用方式相同，为了方便起见，我们将 QTFF 视为 `IsobmffInputFormat` 的实例。
:::

---

---
url: /guide/output-formats.md
---
# 输出格式

## 简介

*输出格式* 指定了由 `Output` 写入的数据容器格式。Mediabunny 支持多种常用容器格式，每种格式都有特定的选项。

许多格式还提供*数据回调*，这些特殊回调会在输出文件的特定数据区域触发。

### 输出格式属性

所有输出格式都有一组通用属性可供查询。

```ts
// 获取格式的文件扩展名：
format.fileExtension; // => '.mp4'

// 获取格式的基础 MIME 类型：
format.mimeType; // => 'video/mp4'

// 检查格式支持的编解码器：
format.getSupportedCodecs(); // => MediaCodec[]
format.getSupportedVideoCodecs(); // => VideoCodec[]
format.getSupportedAudioCodecs(); // => AudioCodec[]
format.getSupportedSubtitleCodecs(); // => SubtitleCodec[]

// 检查格式是否支持带旋转元数据的视频轨道：
format.supportsVideoRotationMetadata; // => boolean
```

请参考[兼容性表格](./supported-formats-and-codecs.md#compatibility-table)查看哪些编解码器可与哪些输出格式配合使用。

不同格式支持的轨道数量和类型也有所不同。您可以通过以下方式获取这些信息：

```ts
format.getSupportedTrackCounts(); // => TrackCountLimits

type TrackCountLimits = {
	video: { min: number, max: number },
	audio: { min: number, max: number },
	subtitle: { min: number, max: number },
	total: { min: number, max: number },
};
```

### 仅追加写入

某些输出格式配置采用\*仅追加(append-only)\*方式进行写入。这意味着它们只会在文件末尾添加新数据，而无需回溯覆盖先前写入的文件部分。或者更正式地说：任何写入操作的字节偏移量正好等于之前写入的字节总数。

仅追加格式与 [`StreamTarget`](./writing-media-files#streamtarget) 结合使用时具有一些实用特性。它们支持与 [媒体源扩展(Media Source Extensions)](https://developer.mozilla.org/en-US/docs/Web/API/Media_Source_Extensions_API) 配合使用，并能轻松实现跨网络流式传输，例如用于文件上传场景。

## MP4 格式

此输出格式用于创建 MP4 文件。

```ts
import { Output, Mp4OutputFormat } from 'mediabunny';

const output = new Output({
	format: new Mp4OutputFormat(options),
	// ...
});
```

可用选项如下：

```ts
type IsobmffOutputFormatOptions = {
	fastStart?: false | 'in-memory' | 'fragmented';
	minimumFragmentDuration?: number;

	onFtyp?: (data: Uint8Array, position: number) => unknown;
	onMoov?: (data: Uint8Array, position: number) => unknown;
	onMdat?: (data: Uint8Array, position: number) => unknown;
	onMoof?: (data: Uint8Array, position: number, timestamp: number) => unknown;
};
```

* `fastStart`\
  控制元数据在文件中的位置。将元数据放在文件开头被称为"快速启动"(Fast Start)，具有以下优势：文件更容易通过网络流式传输而无需范围请求，且YouTube等网站在上传时即可开始处理视频。但将元数据放在文件开头可能会在写入步骤中需要更多处理时间和内存。本库通过设置`fastStart`提供对元数据位置的完全控制：
  * `false`\
    禁用快速启动，将元数据放在文件末尾。速度最快且内存占用最低。
  * `'in-memory'`\
    通过在内存中保留所有媒体块直到文件最终化，生成具有快速启动的文件。这会以更高的最终化步骤成本和内存需求为代价，产生高质量且紧凑的输出。
    ::: info
    此选项确保[仅追加写入](#append-only-writing)，尽管所有写入操作都在最后批量进行。
    :::
  * `'fragmented'`\
    生成\_分段MP4(fMP4)\_文件，通过将样本元数据分组为"片段"(短的媒体段)均匀分布在文件中，同时将通用元数据放在文件开头。分段文件在流式传输场景中非常理想，因为每个片段都可以单独播放而无需了解其他片段。此外，无论文件变得多大，它们都能保持轻量级的创建过程，因为不需要长时间将媒体保留在内存中。但分段文件不如常规MP4文件那样得到广泛和全面的支持，某些播放器不提供对其的搜索功能。
    ::: info
    此选项确保[仅追加写入](#append-only-writing)。
    :::
    ::: warning
    此选项需要[数据包缓冲](./writing-media-files#packet-buffering)。
    :::
  * `undefined`\
    默认选项；当使用[`BufferTarget`](./writing-media-files#buffertarget)时行为类似`'in-memory'`，否则行为类似`false`。
* `minimumFragmentDuration`\
  仅在`fastStart`设为`'fragmented'`时相关。设置片段在被最终化并写入文件前必须具有的最小持续时间(秒)。默认为1秒。
* `onFtyp`\
  在输出文件的ftyp(文件类型)盒子写入完成后调用。
* `onMoov`\
  在输出文件的moov(电影)盒子写入完成后调用。
* `onMdat`\
  对输出文件中每个最终化的mdat(媒体数据)盒子调用。当不使用`fastStart: 'fragmented'`时不推荐使用此回调，因为会有一个庞大的mdat盒子可能需要大量内存。
* `onMoof`\
  对输出文件中每个最终化的moof(电影片段)盒子调用。同时会传递片段的开始时间戳(秒)。

## QuickTime 文件格式 (.mov)

此输出格式用于创建 QuickTime 文件 (.mov)。

```ts
import { Output, MovOutputFormat } from 'mediabunny';

const output = new Output({
	format: new MovOutputFormat(options),
	// ...
});
```

可用选项与 [MP4](#mp4) 使用的 `IsobmffOutputFormatOptions` 相同。

## WebM

此输出格式用于创建 WebM 文件。

```ts
import { Output, WebMOutputFormat } from 'mediabunny';

const output = new Output({
	format: new WebMOutputFormat(options),
	// ...
});
```

以下是可用选项：

```ts
type MkvOutputFormatOptions = {
	appendOnly?: boolean;
	minimumClusterDuration?: number;

	onEbmlHeader?: (data: Uint8Array, position: number) => void;
	onSegmentHeader?: (data: Uint8Array, position: number) => unknown;
	onCluster?: (data: Uint8Array, position: number, timestamp: number) => unknown;
};
```

* `appendOnly`\
  配置输出以仅追加方式写入数据。这对于实时流式传输正在创建的输出非常有用。请注意，启用此选项时，文件时长或跳转等某些功能将被禁用或受到影响，因此当您想写出媒体文件供以后使用时，请不要使用此选项。
  ::: info
  此选项确保 [仅追加写入](#append-only-writing)。
  :::
* `minimumClusterDuration`\
  设置集群必须具有的最小持续时间（以秒为单位）才能完成并写入文件。默认为 1 秒。
* `onEbmlHeader`\
  当输出文件的 EBML 头写入完成后将被调用。
* `onSegmentHeader`\
  当 Matroska Segment 元素的头部部分写入完成后将被调用。头部数据包括 Segment 元素及其内部的所有内容，直到（但不包括）第一个 Matroska Cluster。
* `onCluster`\
  对于输出文件中每个最终确定的 Matroska Cluster 都会被调用。同时会传递集群的开始时间戳（以秒为单位）。

## Matroska (.mkv)

此输出格式用于创建 Matroska 文件 (.mkv)。

```ts
import { Output, MkvOutputFormat } from 'mediabunny';

const output = new Output({
	format: new MkvOutputFormat(options),
	// ...
});
```

可用选项与 [WebM](#webm) 使用的 `MkvOutputFormatOptions` 相同。

## Ogg

此输出格式用于创建 Ogg 文件。

```ts
import { Output, OggOutputFormat } from 'mediabunny';

const output = new Output({
	format: new OggOutputFormat(options),
	// ...
});
```

::: info
此格式确保 [仅追加写入](#append-only-writing)。
:::

可用选项如下：

```ts
type OggOutputFormatOptions = {
	onPage?: (data: Uint8Array, position: number, source: MediaSource) => unknown;
};
```

* `onPage`\
  每个最终确定的 Ogg 页面都会触发此回调。同时会传入支持该页面轨道（逻辑比特流）的 [媒体源](./media-sources)。

## MP3

此输出格式用于创建 MP3 文件。

```ts
import { Output, Mp3OutputFormat } from 'mediabunny';	

const output = new Output({
	format: new Mp3OutputFormat(options),
	// ...
});
```

可用选项如下：

```ts
type Mp3OutputFormatOptions = {
	xingHeader?: boolean;
	onXingFrame?: (data: Uint8Array, position: number) => unknown;
};
```

* `xingHeader`\
  控制是否在 MP3 文件开头写入包含额外元数据和索引的 Xing 头。默认为 `true`。
  ::: info
  当设为 `false` 时，此选项确保 [仅追加写入](#append-only-writing)。
  :::
* `onXingFrame`\
  当 Xing 元数据帧最终确定时会触发此回调（发生在写入过程结束时）。仅当 `xingHeader` 不为 `false` 时才会触发此回调。

::: info
大多数浏览器不支持 MP3 编码。请使用官方的 [`@mediabunny/mp3-encoder`](./extensions/mp3-encoder) 包来填充编码器支持。
:::

## WAVE 格式

此输出格式用于创建 WAVE (.wav) 文件。

```ts
import { Output, WavOutputFormat } from 'mediabunny';	

const output = new Output({
	format: new WavOutputFormat(options),
	// ...
});
```

可用选项如下：

```ts
type WavOutputFormatOptions = {
	large?: boolean;
	onHeader?: (data: Uint8Array, position: number) => unknown;
};
```

* `large`\
  启用后将生成 RF64 格式文件，允许文件大小超过 4 GiB，这是普通 WAVE 文件无法实现的。
* `onHeader`\
  当文件头写入完成后会调用此回调。文件头包含 RIFF 头、格式块和数据块的起始部分（带有占位大小 0）。

## ADTS 格式

此输出格式用于创建 ADTS (.aac) 文件。

```ts
import { Output, AdtsOutputFormat } from 'mediabunny';	

const output = new Output({
	format: new AdtsOutputFormat(options),
	// ...
});
```

可用选项如下：

```ts
type AdtsOutputFormatOptions = {
	onFrame?: (data: Uint8Array, position: number) => unknown;
};
```

* `onFrame`\
  每写入一个 ADTS 帧时都会调用此回调。
